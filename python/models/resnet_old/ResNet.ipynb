{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ===== Import necessary libraries =====\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch.quantization\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# ===== Set up the SimpleNN model =====\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion*out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion*out_channels, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion*out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, architecture_type='cnn', conv_layer_configs=None, fc_layer_configs=None, res_block_configs=None, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Initialization code for other parts of the class remains the same\n",
    "        \n",
    "        if architecture_type == 'cnn':\n",
    "            self.myNetworkType = 'cnn'\n",
    "            self.features = self._make_cnn_layers(conv_layer_configs)\n",
    "            prev_features = conv_layer_configs[-1]['out_channels']\n",
    "        elif architecture_type == 'resnet':\n",
    "            self.myNetworkType = 'resnet'\n",
    "            self.init_conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.init_bn = nn.BatchNorm2d(16)\n",
    "            self.init_relu = nn.ReLU(inplace=True)\n",
    "            self.features, prev_features = self._make_resnet_layers(res_block_configs)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type: {}\".format(architecture_type))\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = self._make_fc_layers(prev_features, fc_layer_configs, num_classes)\n",
    "\n",
    "    def _make_cnn_layers(self, conv_layer_configs):\n",
    "        layers = []\n",
    "        for conv_layer in conv_layer_configs:\n",
    "            in_channels = conv_layer['in_channels']\n",
    "            out_channels = conv_layer['out_channels']\n",
    "            kernel_size = conv_layer['kernel_size']\n",
    "            stride = conv_layer['stride']\n",
    "            padding = conv_layer['padding']\n",
    "            block = CNNBlock(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            layers.append(block)\n",
    "        in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _make_resnet_layers(self, res_block_configs):\n",
    "        layers = []\n",
    "        for block_config in res_block_configs:\n",
    "            in_channels = block_config['in_channels']\n",
    "            out_channels = block_config['out_channels']\n",
    "            num_blocks = block_config['num_blocks']\n",
    "            stride = block_config['stride']\n",
    "            \n",
    "            layers.append(self._make_layer(ResBlock, in_channels, out_channels, num_blocks, stride))\n",
    "            \n",
    "            # Update in_channels for the next set of blocks\n",
    "            in_channels = out_channels * ResBlock.expansion\n",
    "        return nn.Sequential(*layers), in_channels\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)  # First block might have a stride to downsample\n",
    "        blocks = []\n",
    "        for stride in strides:\n",
    "            blocks.append(block(in_channels, out_channels, stride))\n",
    "            in_channels = out_channels * block.expansion  # Update in_channels for the next block\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def _make_fc_layers(self, prev_features, fc_layer_configs, num_classes):\n",
    "        layers = nn.ModuleList()\n",
    "        if fc_layer_configs is not None:\n",
    "            for fc_layer in fc_layer_configs:\n",
    "                layers.append(nn.Linear(prev_features, fc_layer['out_features']))\n",
    "                prev_features = fc_layer['out_features']\n",
    "            layers.append(nn.Linear(prev_features, num_classes))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class QuantizableCNN(CNN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(QuantizableCNN, self).__init__(*args, **kwargs)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ===== Define and set HyperParameters =====\n",
    "\n",
    "\n",
    "## DataLoader\n",
    "TRAIN_BATCH_SIZE = 64  # training batch size\n",
    "VAL_BATCH_SIZE = 50  # validation batch size\n",
    "NUM_WORKERS = 8  # number of workers for DataLoader\n",
    "\n",
    "## Model\n",
    "### CNN\n",
    "conv_layer_configs = [\n",
    "    {'in_channels': 3, 'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'in_channels': 32, 'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'in_channels': 64, 'out_channels': 128, 'kernel_size': 3, 'stride': 2, 'padding': 1}\n",
    "]\n",
    "\n",
    "fc_layer_configs = [\n",
    "    {'out_features': 256},\n",
    "    {'out_features': 128}\n",
    "]\n",
    "\n",
    "### ResNet\n",
    "res_block_configs = [\n",
    "    {'in_channels': 3, 'out_channels': 16, 'stride': 1, 'num_blocks': 3},\n",
    "    {'in_channels': 16, 'out_channels': 32, 'stride': 2, 'num_blocks': 3},\n",
    "    {'in_channels': 32, 'out_channels': 64, 'stride': 2, 'num_blocks': 3}\n",
    "]\n",
    "\n",
    "## Optimizer and scheduler\n",
    "INITIAL_LR = 0.1  # initial learning rate\n",
    "MOMENTUM = 0.9  # momentum for optimizer\n",
    "REG = 1e-4  # L2 regularization strength\n",
    "LR_PATIENCE = 5  # Patience for ReduceLROnPlateau scheduler\n",
    "LR_FACTOR = 0.25  # Factor by which the learning rate will be reduced\n",
    "\n",
    "## Training\n",
    "EPOCHS = 3  # total number of training epochs\n",
    "CHECKPOINT_FOLDER = \"./saved_models\"  # folder where models are saved\n",
    "\n",
    "## Pruning and Quantization\n",
    "ENABLE_QUANTIZATION = True\n",
    "\n",
    "# ===== Set up preprocessing functions =====\n",
    "\n",
    "\n",
    "## specify preprocessing function\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "\n",
    "# ===== Set up dataset and dataloader =====\n",
    "\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "\n",
    "## construct dataset\n",
    "train_set = CIFAR10(\n",
    "    root = DATA_ROOT, \n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform_train\n",
    ")\n",
    "val_set = CIFAR10(\n",
    "    root = DATA_ROOT, \n",
    "    train = False, \n",
    "    download = True,\n",
    "    transform = transform_val\n",
    ")\n",
    "\n",
    "## construct dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size= TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# ===== Instantiate your SimpleNN model and deploy it to device =====\n",
    "\n",
    "\n",
    "## specify the device for computation\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if ENABLE_QUANTIZATION:\n",
    "    net = QuantizableCNN(architecture_type='resnet', res_block_configs=res_block_configs, num_classes=10)\n",
    "    net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')  # Use 'fbgemm' for x86 or 'qnnpack' for ARM\n",
    "    torch.quantization.prepare_qat(net, inplace=True)\n",
    "else:\n",
    "    # For CNN\n",
    "    # net = CNN(architecture_type='cnn', conv_layer_configs=conv_layer_configs, fc_layer_configs=fc_layer_configs, num_classes=10)\n",
    "    # For ResNet\n",
    "    net = CNN(architecture_type='resnet', res_block_configs=res_block_configs, num_classes=10)\n",
    "\n",
    "# deploy the network to device\n",
    "net.to(device)\n",
    "\n",
    "print(next(net.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "# ===== Set up the loss function and optimizer =====\n",
    "\n",
    "\n",
    "## loss function\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "## Add optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=LR_PATIENCE, factor=LR_FACTOR)\n",
    "\n",
    "\n",
    "# ===== Start the training process =====\n",
    "\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"==> Training starts!\")\n",
    "print(\"=\"*50)\n",
    "for i in range(0, EPOCHS):    \n",
    "    ## switch to train mode\n",
    "    net.train()\n",
    "\n",
    "    ## print the Epoch and learning rate\n",
    "    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {i}: with learning rate {current_learning_rate}\")\n",
    "    \n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    ## Train the model\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        ### copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        ### compute the output and loss\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets.long())\n",
    "        \n",
    "        ### zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ### apply gradient and update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### count the number of correctly predicted samples in the current batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_examples += targets.size(0)\n",
    "        correct_examples += (predicted == targets).sum().item()\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "                \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "\n",
    "    ## Validate on the validation dataset\n",
    "    ## switch to eval mode\n",
    "    net.eval()\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    ## disable gradient during validation, which can save GPU memory\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            ### copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            ### compute the output and loss\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets.long())\n",
    "            \n",
    "            ### count the number of correctly predicted samples in the current batch\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_examples += targets.size(0)\n",
    "            correct_examples += (predicted == targets).sum().item()\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "\n",
    "    ## decay learning rate\n",
    "    previous_learning_rate = current_learning_rate\n",
    "    scheduler.step(val_loss)\n",
    "    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "    if previous_learning_rate != current_learning_rate:\n",
    "        print(f\"Learning rate decayed to {current_learning_rate}\")\n",
    "    \n",
    "    ## save the model checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "           os.makedirs(CHECKPOINT_FOLDER)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'state_dict': net.state_dict(),\n",
    "                'epoch': i,\n",
    "                'lr': current_learning_rate}\n",
    "        saveName = 'CNN_quantizeTrained' if ENABLE_QUANTIZATION else 'CNN'\n",
    "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, saveName + '.pth'))\n",
    "    print('')\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_quantized_model(modelPath, quantizeTrained):\n",
    "    if quantizeTrained:\n",
    "        model = QuantizableCNN(architecture_type='resnet', res_block_configs=res_block_configs, num_classes=10)\n",
    "        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "        torch.quantization.prepare_qat(model, inplace=True)\n",
    "    else:\n",
    "        model = CNN(architecture_type='resnet', res_block_configs=res_block_configs, num_classes=10)\n",
    "\n",
    "    checkpoint = torch.load(modelPath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"Before quantization: Checking model structure and qconfig compatibility\")\n",
    "    torch.quantization.convert(model.eval(), inplace=True)\n",
    "    print(\"Quantization successful\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before quantization: Checking model structure and qconfig compatibility\n",
      "Quantization successful\n",
      "Quantized Model Size: 0.34 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\ao\\quantization\\utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "quantizedModel = True\n",
    "model = convert_to_quantized_model(os.path.join(CHECKPOINT_FOLDER, 'CNN_quantizeTrained.pth'), quantizedModel)\n",
    "\n",
    "saveName = 'CNN' + '_quantized' if quantizedModel else 'CNN'\n",
    "torch.save(model.state_dict(), os.path.join(CHECKPOINT_FOLDER, saveName + '.pth'))\n",
    "\n",
    "# Check the size of the quantized model\n",
    "model_size_bytes = os.path.getsize(CHECKPOINT_FOLDER + '/' + saveName + '.pth')\n",
    "model_size_mb = model_size_bytes / (1024 * 1024)  # Convert bytes to megabytes\n",
    "print(f\"Quantized Model Size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
