{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda:0\n",
      "==> Training starts!\n",
      "==================================================\n",
      "Epoch 0: with learning rate 0.1\n",
      "Training loss: 108.4855, Training accuracy: 0.3881\n",
      "Validation loss: 72.4654, Validation accuracy: 0.4967\n",
      "Saving ...\n",
      "\n",
      "Epoch 1: with learning rate 0.1\n",
      "Training loss: 79.2885, Training accuracy: 0.5547\n",
      "Validation loss: 72.6809, Validation accuracy: 0.5462\n",
      "Saving ...\n",
      "\n",
      "Epoch 2: with learning rate 0.1\n",
      "Training loss: 67.0786, Training accuracy: 0.6268\n",
      "Validation loss: 51.7093, Validation accuracy: 0.6517\n",
      "Saving ...\n",
      "\n",
      "Epoch 3: with learning rate 0.1\n",
      "Training loss: 58.9287, Training accuracy: 0.6753\n",
      "Validation loss: 45.7523, Validation accuracy: 0.6921\n",
      "Saving ...\n",
      "\n",
      "Epoch 4: with learning rate 0.1\n",
      "Training loss: 54.0803, Training accuracy: 0.7044\n",
      "Validation loss: 42.6750, Validation accuracy: 0.7235\n",
      "Saving ...\n",
      "\n",
      "Epoch 5: with learning rate 0.1\n",
      "Training loss: 50.3777, Training accuracy: 0.7253\n",
      "Validation loss: 41.4692, Validation accuracy: 0.7318\n",
      "Saving ...\n",
      "\n",
      "Epoch 6: with learning rate 0.1\n",
      "Training loss: 47.9787, Training accuracy: 0.7394\n",
      "Validation loss: 33.8418, Validation accuracy: 0.7638\n",
      "Saving ...\n",
      "\n",
      "Epoch 7: with learning rate 0.1\n",
      "Training loss: 46.0424, Training accuracy: 0.7481\n",
      "Validation loss: 33.0358, Validation accuracy: 0.7738\n",
      "Saving ...\n",
      "\n",
      "Epoch 8: with learning rate 0.1\n",
      "Training loss: 43.7589, Training accuracy: 0.7639\n",
      "Validation loss: 35.6082, Validation accuracy: 0.7602\n",
      "\n",
      "Epoch 9: with learning rate 0.1\n",
      "Training loss: 42.5932, Training accuracy: 0.7697\n",
      "Validation loss: 34.3614, Validation accuracy: 0.7695\n",
      "\n",
      "Epoch 10: with learning rate 0.1\n",
      "Training loss: 41.1428, Training accuracy: 0.7789\n",
      "Validation loss: 37.1377, Validation accuracy: 0.7624\n",
      "\n",
      "Epoch 11: with learning rate 0.1\n",
      "Training loss: 40.3709, Training accuracy: 0.7816\n",
      "Validation loss: 30.6881, Validation accuracy: 0.7927\n",
      "Saving ...\n",
      "\n",
      "Epoch 12: with learning rate 0.1\n",
      "Training loss: 39.1624, Training accuracy: 0.7894\n",
      "Validation loss: 32.8846, Validation accuracy: 0.7798\n",
      "\n",
      "Epoch 13: with learning rate 0.1\n",
      "Training loss: 38.5204, Training accuracy: 0.7913\n",
      "Validation loss: 36.0466, Validation accuracy: 0.7602\n",
      "\n",
      "Epoch 14: with learning rate 0.1\n",
      "Training loss: 38.0525, Training accuracy: 0.7958\n",
      "Validation loss: 27.0525, Validation accuracy: 0.8172\n",
      "Saving ...\n",
      "\n",
      "Epoch 15: with learning rate 0.1\n",
      "Training loss: 37.6338, Training accuracy: 0.8000\n",
      "Validation loss: 36.1149, Validation accuracy: 0.7702\n",
      "\n",
      "Epoch 16: with learning rate 0.1\n",
      "Training loss: 36.6521, Training accuracy: 0.8018\n",
      "Validation loss: 27.7669, Validation accuracy: 0.8094\n",
      "\n",
      "Epoch 17: with learning rate 0.1\n",
      "Training loss: 36.3372, Training accuracy: 0.8065\n",
      "Validation loss: 28.1197, Validation accuracy: 0.8155\n",
      "\n",
      "Epoch 18: with learning rate 0.1\n",
      "Training loss: 35.3762, Training accuracy: 0.8098\n",
      "Validation loss: 26.8355, Validation accuracy: 0.8138\n",
      "\n",
      "Epoch 19: with learning rate 0.1\n",
      "Training loss: 35.0625, Training accuracy: 0.8106\n",
      "Validation loss: 27.3958, Validation accuracy: 0.8105\n",
      "\n",
      "Epoch 20: with learning rate 0.1\n",
      "Training loss: 34.8632, Training accuracy: 0.8124\n",
      "Validation loss: 28.4082, Validation accuracy: 0.8033\n",
      "\n",
      "Epoch 21: with learning rate 0.1\n",
      "Training loss: 34.7075, Training accuracy: 0.8133\n",
      "Validation loss: 25.8952, Validation accuracy: 0.8237\n",
      "Saving ...\n",
      "\n",
      "Epoch 22: with learning rate 0.1\n",
      "Training loss: 33.8098, Training accuracy: 0.8161\n",
      "Validation loss: 27.3288, Validation accuracy: 0.8209\n",
      "\n",
      "Epoch 23: with learning rate 0.1\n",
      "Training loss: 33.7751, Training accuracy: 0.8190\n",
      "Validation loss: 26.3643, Validation accuracy: 0.8251\n",
      "Saving ...\n",
      "\n",
      "Epoch 24: with learning rate 0.1\n",
      "Training loss: 33.2727, Training accuracy: 0.8208\n",
      "Validation loss: 29.6812, Validation accuracy: 0.7999\n",
      "\n",
      "Epoch 25: with learning rate 0.1\n",
      "Training loss: 33.1433, Training accuracy: 0.8225\n",
      "Validation loss: 24.9912, Validation accuracy: 0.8336\n",
      "Saving ...\n",
      "\n",
      "Epoch 26: with learning rate 0.1\n",
      "Training loss: 32.8343, Training accuracy: 0.8232\n",
      "Validation loss: 24.0184, Validation accuracy: 0.8362\n",
      "Saving ...\n",
      "\n",
      "Epoch 27: with learning rate 0.1\n",
      "Training loss: 32.5954, Training accuracy: 0.8251\n",
      "Validation loss: 25.8553, Validation accuracy: 0.8305\n",
      "\n",
      "Epoch 28: with learning rate 0.1\n",
      "Training loss: 32.1292, Training accuracy: 0.8269\n",
      "Validation loss: 31.3773, Validation accuracy: 0.8000\n",
      "\n",
      "Epoch 29: with learning rate 0.1\n",
      "Training loss: 32.0996, Training accuracy: 0.8267\n",
      "Validation loss: 24.2496, Validation accuracy: 0.8350\n",
      "\n",
      "Epoch 30: with learning rate 0.1\n",
      "Training loss: 31.8195, Training accuracy: 0.8297\n",
      "Validation loss: 26.1805, Validation accuracy: 0.8251\n",
      "\n",
      "Epoch 31: with learning rate 0.1\n",
      "Training loss: 31.8721, Training accuracy: 0.8272\n",
      "Validation loss: 26.0035, Validation accuracy: 0.8237\n",
      "\n",
      "Epoch 32: with learning rate 0.1\n",
      "Training loss: 31.5341, Training accuracy: 0.8286\n",
      "Validation loss: 27.3158, Validation accuracy: 0.8162\n",
      "Learning rate decayed to 0.025\n",
      "\n",
      "Epoch 33: with learning rate 0.025\n",
      "Training loss: 24.0532, Training accuracy: 0.8715\n",
      "Validation loss: 17.0761, Validation accuracy: 0.8836\n",
      "Saving ...\n",
      "\n",
      "Epoch 34: with learning rate 0.025\n",
      "Training loss: 22.3467, Training accuracy: 0.8793\n",
      "Validation loss: 16.9380, Validation accuracy: 0.8852\n",
      "Saving ...\n",
      "\n",
      "Epoch 35: with learning rate 0.025\n",
      "Training loss: 21.9152, Training accuracy: 0.8825\n",
      "Validation loss: 17.3073, Validation accuracy: 0.8850\n",
      "\n",
      "Epoch 36: with learning rate 0.025\n",
      "Training loss: 21.2013, Training accuracy: 0.8842\n",
      "Validation loss: 16.8410, Validation accuracy: 0.8878\n",
      "Saving ...\n",
      "\n",
      "Epoch 37: with learning rate 0.025\n",
      "Training loss: 21.1406, Training accuracy: 0.8851\n",
      "Validation loss: 17.0378, Validation accuracy: 0.8863\n",
      "\n",
      "Epoch 38: with learning rate 0.025\n",
      "Training loss: 20.7403, Training accuracy: 0.8873\n",
      "Validation loss: 17.1934, Validation accuracy: 0.8830\n",
      "\n",
      "Epoch 39: with learning rate 0.025\n",
      "Training loss: 20.6402, Training accuracy: 0.8880\n",
      "Validation loss: 16.4052, Validation accuracy: 0.8882\n",
      "Saving ...\n",
      "\n",
      "Epoch 40: with learning rate 0.025\n",
      "Training loss: 20.7008, Training accuracy: 0.8886\n",
      "Validation loss: 17.3887, Validation accuracy: 0.8832\n",
      "\n",
      "Epoch 41: with learning rate 0.025\n",
      "Training loss: 20.1237, Training accuracy: 0.8904\n",
      "Validation loss: 16.9957, Validation accuracy: 0.8841\n",
      "\n",
      "Epoch 42: with learning rate 0.025\n",
      "Training loss: 20.4812, Training accuracy: 0.8890\n",
      "Validation loss: 16.3776, Validation accuracy: 0.8894\n",
      "Saving ...\n",
      "\n",
      "Epoch 43: with learning rate 0.025\n",
      "Training loss: 20.5751, Training accuracy: 0.8879\n",
      "Validation loss: 17.2317, Validation accuracy: 0.8865\n",
      "\n",
      "Epoch 44: with learning rate 0.025\n",
      "Training loss: 20.3325, Training accuracy: 0.8899\n",
      "Validation loss: 16.8656, Validation accuracy: 0.8923\n",
      "Saving ...\n",
      "\n",
      "Epoch 45: with learning rate 0.025\n",
      "Training loss: 19.8919, Training accuracy: 0.8927\n",
      "Validation loss: 19.8383, Validation accuracy: 0.8716\n",
      "\n",
      "Epoch 46: with learning rate 0.025\n",
      "Training loss: 20.0535, Training accuracy: 0.8931\n",
      "Validation loss: 17.8730, Validation accuracy: 0.8873\n",
      "\n",
      "Epoch 47: with learning rate 0.025\n",
      "Training loss: 20.1348, Training accuracy: 0.8916\n",
      "Validation loss: 16.8176, Validation accuracy: 0.8906\n",
      "\n",
      "Epoch 48: with learning rate 0.025\n",
      "Training loss: 20.0989, Training accuracy: 0.8920\n",
      "Validation loss: 18.4448, Validation accuracy: 0.8768\n",
      "Learning rate decayed to 0.00625\n",
      "\n",
      "Epoch 49: with learning rate 0.00625\n",
      "Training loss: 16.9708, Training accuracy: 0.9091\n",
      "Validation loss: 14.8715, Validation accuracy: 0.9017\n",
      "Saving ...\n",
      "\n",
      "Epoch 50: with learning rate 0.00625\n",
      "Training loss: 15.7638, Training accuracy: 0.9157\n",
      "Validation loss: 14.8557, Validation accuracy: 0.9016\n",
      "\n",
      "Epoch 51: with learning rate 0.00625\n",
      "Training loss: 15.6464, Training accuracy: 0.9152\n",
      "Validation loss: 14.8616, Validation accuracy: 0.9006\n",
      "\n",
      "Epoch 52: with learning rate 0.00625\n",
      "Training loss: 15.3257, Training accuracy: 0.9180\n",
      "Validation loss: 14.7488, Validation accuracy: 0.9045\n",
      "Saving ...\n",
      "\n",
      "Epoch 53: with learning rate 0.00625\n",
      "Training loss: 15.1698, Training accuracy: 0.9183\n",
      "Validation loss: 14.4177, Validation accuracy: 0.9065\n",
      "Saving ...\n",
      "\n",
      "Epoch 54: with learning rate 0.00625\n",
      "Training loss: 14.8774, Training accuracy: 0.9201\n",
      "Validation loss: 14.5225, Validation accuracy: 0.9011\n",
      "\n",
      "Epoch 55: with learning rate 0.00625\n",
      "Training loss: 14.9494, Training accuracy: 0.9203\n",
      "Validation loss: 15.0878, Validation accuracy: 0.9006\n",
      "\n",
      "Epoch 56: with learning rate 0.00625\n",
      "Training loss: 14.7801, Training accuracy: 0.9206\n",
      "Validation loss: 15.1327, Validation accuracy: 0.8986\n",
      "\n",
      "Epoch 57: with learning rate 0.00625\n",
      "Training loss: 14.7705, Training accuracy: 0.9205\n",
      "Validation loss: 15.0323, Validation accuracy: 0.9043\n",
      "\n",
      "Epoch 58: with learning rate 0.00625\n",
      "Training loss: 14.3731, Training accuracy: 0.9223\n",
      "Validation loss: 14.8971, Validation accuracy: 0.9055\n",
      "\n",
      "Epoch 59: with learning rate 0.00625\n",
      "Training loss: 14.5669, Training accuracy: 0.9224\n",
      "Validation loss: 15.5093, Validation accuracy: 0.9030\n",
      "Learning rate decayed to 0.0015625\n",
      "\n",
      "Epoch 60: with learning rate 0.0015625\n",
      "Training loss: 13.6900, Training accuracy: 0.9261\n",
      "Validation loss: 14.4162, Validation accuracy: 0.9071\n",
      "Saving ...\n",
      "\n",
      "Epoch 61: with learning rate 0.0015625\n",
      "Training loss: 13.2845, Training accuracy: 0.9283\n",
      "Validation loss: 14.4605, Validation accuracy: 0.9072\n",
      "Saving ...\n",
      "\n",
      "Epoch 62: with learning rate 0.0015625\n",
      "Training loss: 12.9163, Training accuracy: 0.9317\n",
      "Validation loss: 14.4053, Validation accuracy: 0.9052\n",
      "\n",
      "Epoch 63: with learning rate 0.0015625\n",
      "Training loss: 12.9813, Training accuracy: 0.9303\n",
      "Validation loss: 14.2080, Validation accuracy: 0.9090\n",
      "Saving ...\n",
      "\n",
      "Epoch 64: with learning rate 0.0015625\n",
      "Training loss: 12.7522, Training accuracy: 0.9321\n",
      "Validation loss: 14.2574, Validation accuracy: 0.9088\n",
      "\n",
      "Epoch 65: with learning rate 0.0015625\n",
      "Training loss: 12.7237, Training accuracy: 0.9322\n",
      "Validation loss: 14.3609, Validation accuracy: 0.9069\n",
      "\n",
      "Epoch 66: with learning rate 0.0015625\n",
      "Training loss: 12.8885, Training accuracy: 0.9312\n",
      "Validation loss: 14.3136, Validation accuracy: 0.9090\n",
      "\n",
      "Epoch 67: with learning rate 0.0015625\n",
      "Training loss: 12.8332, Training accuracy: 0.9306\n",
      "Validation loss: 14.2916, Validation accuracy: 0.9080\n",
      "\n",
      "Epoch 68: with learning rate 0.0015625\n",
      "Training loss: 12.4214, Training accuracy: 0.9339\n",
      "Validation loss: 14.2740, Validation accuracy: 0.9065\n",
      "\n",
      "Epoch 69: with learning rate 0.0015625\n",
      "Training loss: 12.6478, Training accuracy: 0.9323\n",
      "Validation loss: 14.3168, Validation accuracy: 0.9077\n",
      "Learning rate decayed to 0.000390625\n",
      "\n",
      "Epoch 70: with learning rate 0.000390625\n",
      "Training loss: 12.5510, Training accuracy: 0.9328\n",
      "Validation loss: 14.3809, Validation accuracy: 0.9070\n",
      "\n",
      "Epoch 71: with learning rate 0.000390625\n",
      "Training loss: 12.2983, Training accuracy: 0.9350\n",
      "Validation loss: 14.3018, Validation accuracy: 0.9080\n",
      "\n",
      "Epoch 72: with learning rate 0.000390625\n",
      "Training loss: 12.2035, Training accuracy: 0.9347\n",
      "Validation loss: 14.2866, Validation accuracy: 0.9069\n",
      "\n",
      "Epoch 73: with learning rate 0.000390625\n",
      "Training loss: 12.3582, Training accuracy: 0.9333\n",
      "Validation loss: 14.2714, Validation accuracy: 0.9071\n",
      "\n",
      "Epoch 74: with learning rate 0.000390625\n",
      "Training loss: 12.3016, Training accuracy: 0.9345\n",
      "Validation loss: 14.2547, Validation accuracy: 0.9099\n",
      "Saving ...\n",
      "\n",
      "Epoch 75: with learning rate 0.000390625\n",
      "Training loss: 12.0438, Training accuracy: 0.9346\n",
      "Validation loss: 14.2313, Validation accuracy: 0.9109\n",
      "Learning rate decayed to 9.765625e-05\n",
      "Saving ...\n",
      "\n",
      "Epoch 76: with learning rate 9.765625e-05\n",
      "Training loss: 12.1331, Training accuracy: 0.9354\n",
      "Validation loss: 14.1876, Validation accuracy: 0.9095\n",
      "\n",
      "Epoch 77: with learning rate 9.765625e-05\n",
      "Training loss: 12.3744, Training accuracy: 0.9338\n",
      "Validation loss: 14.0941, Validation accuracy: 0.9090\n",
      "\n",
      "Epoch 78: with learning rate 9.765625e-05\n",
      "Training loss: 12.2661, Training accuracy: 0.9347\n",
      "Validation loss: 14.1727, Validation accuracy: 0.9089\n",
      "\n",
      "Epoch 79: with learning rate 9.765625e-05\n",
      "Training loss: 12.1797, Training accuracy: 0.9341\n",
      "Validation loss: 14.0968, Validation accuracy: 0.9087\n",
      "\n",
      "Epoch 80: with learning rate 9.765625e-05\n",
      "Training loss: 12.0901, Training accuracy: 0.9365\n",
      "Validation loss: 14.0681, Validation accuracy: 0.9093\n",
      "\n",
      "Epoch 81: with learning rate 9.765625e-05\n",
      "Training loss: 12.4487, Training accuracy: 0.9346\n",
      "Validation loss: 14.2099, Validation accuracy: 0.9094\n",
      "\n",
      "Epoch 82: with learning rate 9.765625e-05\n",
      "Training loss: 12.0978, Training accuracy: 0.9351\n",
      "Validation loss: 14.0113, Validation accuracy: 0.9110\n",
      "Saving ...\n",
      "\n",
      "Epoch 83: with learning rate 9.765625e-05\n",
      "Training loss: 11.9863, Training accuracy: 0.9347\n",
      "Validation loss: 14.1547, Validation accuracy: 0.9079\n",
      "\n",
      "Epoch 84: with learning rate 9.765625e-05\n",
      "Training loss: 11.9052, Training accuracy: 0.9365\n",
      "Validation loss: 14.1164, Validation accuracy: 0.9086\n",
      "\n",
      "Epoch 85: with learning rate 9.765625e-05\n",
      "Training loss: 11.8886, Training accuracy: 0.9367\n",
      "Validation loss: 14.4973, Validation accuracy: 0.9075\n",
      "\n",
      "Epoch 86: with learning rate 9.765625e-05\n",
      "Training loss: 12.3322, Training accuracy: 0.9332\n",
      "Validation loss: 14.1499, Validation accuracy: 0.9098\n",
      "\n",
      "Epoch 87: with learning rate 9.765625e-05\n",
      "Training loss: 12.1750, Training accuracy: 0.9345\n",
      "Validation loss: 14.1572, Validation accuracy: 0.9087\n",
      "\n",
      "Epoch 88: with learning rate 9.765625e-05\n",
      "Training loss: 12.0133, Training accuracy: 0.9355\n",
      "Validation loss: 14.2377, Validation accuracy: 0.9088\n",
      "Learning rate decayed to 2.44140625e-05\n",
      "\n",
      "Epoch 89: with learning rate 2.44140625e-05\n",
      "Training loss: 12.0797, Training accuracy: 0.9355\n",
      "Validation loss: 14.0352, Validation accuracy: 0.9096\n",
      "\n",
      "Epoch 90: with learning rate 2.44140625e-05\n",
      "Training loss: 12.1874, Training accuracy: 0.9350\n",
      "Validation loss: 14.1749, Validation accuracy: 0.9082\n",
      "\n",
      "Epoch 91: with learning rate 2.44140625e-05\n",
      "Training loss: 12.1176, Training accuracy: 0.9344\n",
      "Validation loss: 14.1340, Validation accuracy: 0.9094\n",
      "\n",
      "Epoch 92: with learning rate 2.44140625e-05\n",
      "Training loss: 12.1473, Training accuracy: 0.9343\n",
      "Validation loss: 14.1401, Validation accuracy: 0.9091\n",
      "\n",
      "Epoch 93: with learning rate 2.44140625e-05\n",
      "Training loss: 12.2426, Training accuracy: 0.9345\n",
      "Validation loss: 14.1846, Validation accuracy: 0.9096\n",
      "\n",
      "Epoch 94: with learning rate 2.44140625e-05\n",
      "Training loss: 11.9883, Training accuracy: 0.9349\n",
      "Validation loss: 14.1161, Validation accuracy: 0.9103\n",
      "Learning rate decayed to 6.103515625e-06\n",
      "\n",
      "Epoch 95: with learning rate 6.103515625e-06\n",
      "Training loss: 11.9777, Training accuracy: 0.9365\n",
      "Validation loss: 14.1513, Validation accuracy: 0.9087\n",
      "\n",
      "Epoch 96: with learning rate 6.103515625e-06\n",
      "Training loss: 12.0887, Training accuracy: 0.9351\n",
      "Validation loss: 13.9903, Validation accuracy: 0.9114\n",
      "Saving ...\n",
      "\n",
      "Epoch 97: with learning rate 6.103515625e-06\n",
      "Training loss: 11.8504, Training accuracy: 0.9370\n",
      "Validation loss: 14.1012, Validation accuracy: 0.9107\n",
      "\n",
      "Epoch 98: with learning rate 6.103515625e-06\n",
      "Training loss: 12.3149, Training accuracy: 0.9337\n",
      "Validation loss: 14.1141, Validation accuracy: 0.9090\n",
      "\n",
      "Epoch 99: with learning rate 6.103515625e-06\n",
      "Training loss: 12.0851, Training accuracy: 0.9352\n",
      "Validation loss: 14.1105, Validation accuracy: 0.9091\n",
      "\n",
      "Epoch 100: with learning rate 6.103515625e-06\n",
      "Training loss: 12.1985, Training accuracy: 0.9358\n",
      "Validation loss: 14.1598, Validation accuracy: 0.9088\n",
      "\n",
      "Epoch 101: with learning rate 6.103515625e-06\n",
      "Training loss: 12.0674, Training accuracy: 0.9342\n",
      "Validation loss: 14.2310, Validation accuracy: 0.9070\n",
      "\n",
      "Epoch 102: with learning rate 6.103515625e-06\n",
      "Training loss: 12.0931, Training accuracy: 0.9359\n",
      "Validation loss: 14.2721, Validation accuracy: 0.9084\n",
      "Learning rate decayed to 1.52587890625e-06\n",
      "\n",
      "Epoch 103: with learning rate 1.52587890625e-06\n",
      "Training loss: 12.0395, Training accuracy: 0.9355\n",
      "Validation loss: 14.1512, Validation accuracy: 0.9096\n",
      "\n",
      "Epoch 104: with learning rate 1.52587890625e-06\n",
      "Training loss: 11.9804, Training accuracy: 0.9349\n",
      "Validation loss: 14.1659, Validation accuracy: 0.9097\n",
      "\n",
      "Epoch 105: with learning rate 1.52587890625e-06\n",
      "Training loss: 12.1330, Training accuracy: 0.9356\n",
      "Validation loss: 14.3280, Validation accuracy: 0.9067\n",
      "\n",
      "Epoch 106: with learning rate 1.52587890625e-06\n",
      "Training loss: 12.2167, Training accuracy: 0.9347\n",
      "Validation loss: 14.2863, Validation accuracy: 0.9091\n",
      "\n",
      "Epoch 107: with learning rate 1.52587890625e-06\n",
      "Training loss: 12.1479, Training accuracy: 0.9360\n",
      "Validation loss: 14.3483, Validation accuracy: 0.9075\n",
      "\n",
      "Epoch 108: with learning rate 1.52587890625e-06\n",
      "Training loss: 11.8960, Training accuracy: 0.9355\n",
      "Validation loss: 14.1995, Validation accuracy: 0.9088\n",
      "Learning rate decayed to 3.814697265625e-07\n",
      "\n",
      "Epoch 109: with learning rate 3.814697265625e-07\n",
      "Training loss: 12.0068, Training accuracy: 0.9358\n",
      "Validation loss: 14.0849, Validation accuracy: 0.9088\n",
      "\n",
      "Epoch 110: with learning rate 3.814697265625e-07\n",
      "Training loss: 12.0629, Training accuracy: 0.9353\n",
      "Validation loss: 14.1683, Validation accuracy: 0.9088\n",
      "\n",
      "Epoch 111: with learning rate 3.814697265625e-07\n",
      "Training loss: 12.0285, Training accuracy: 0.9353\n",
      "Validation loss: 14.0749, Validation accuracy: 0.9102\n",
      "\n",
      "Epoch 112: with learning rate 3.814697265625e-07\n",
      "Training loss: 11.9758, Training accuracy: 0.9369\n",
      "Validation loss: 14.1583, Validation accuracy: 0.9096\n",
      "\n",
      "Epoch 113: with learning rate 3.814697265625e-07\n",
      "Training loss: 12.0585, Training accuracy: 0.9359\n",
      "Validation loss: 14.2819, Validation accuracy: 0.9074\n",
      "\n",
      "Epoch 114: with learning rate 3.814697265625e-07\n",
      "Training loss: 11.9831, Training accuracy: 0.9368\n",
      "Validation loss: 14.2274, Validation accuracy: 0.9083\n",
      "Learning rate decayed to 9.5367431640625e-08\n",
      "\n",
      "Epoch 115: with learning rate 9.5367431640625e-08\n",
      "Training loss: 12.0083, Training accuracy: 0.9341\n",
      "Validation loss: 14.0833, Validation accuracy: 0.9103\n",
      "\n",
      "Epoch 116: with learning rate 9.5367431640625e-08\n",
      "Training loss: 12.0584, Training accuracy: 0.9357\n",
      "Validation loss: 14.4440, Validation accuracy: 0.9078\n",
      "\n",
      "Epoch 117: with learning rate 9.5367431640625e-08\n",
      "Training loss: 12.1387, Training accuracy: 0.9354\n",
      "Validation loss: 14.1653, Validation accuracy: 0.9090\n",
      "\n",
      "Epoch 118: with learning rate 9.5367431640625e-08\n",
      "Training loss: 11.8596, Training accuracy: 0.9364\n",
      "Validation loss: 14.0988, Validation accuracy: 0.9101\n",
      "\n",
      "Epoch 119: with learning rate 9.5367431640625e-08\n",
      "Training loss: 12.0855, Training accuracy: 0.9358\n",
      "Validation loss: 14.1809, Validation accuracy: 0.9080\n",
      "\n",
      "Epoch 120: with learning rate 9.5367431640625e-08\n",
      "Training loss: 11.9642, Training accuracy: 0.9361\n",
      "Validation loss: 14.1605, Validation accuracy: 0.9091\n",
      "Learning rate decayed to 2.384185791015625e-08\n",
      "\n",
      "Epoch 121: with learning rate 2.384185791015625e-08\n",
      "Training loss: 12.0567, Training accuracy: 0.9365\n",
      "Validation loss: 14.2206, Validation accuracy: 0.9091\n",
      "\n",
      "Epoch 122: with learning rate 2.384185791015625e-08\n",
      "Training loss: 12.0738, Training accuracy: 0.9366\n",
      "Validation loss: 14.1034, Validation accuracy: 0.9095\n",
      "\n",
      "Epoch 123: with learning rate 2.384185791015625e-08\n",
      "Training loss: 11.8677, Training accuracy: 0.9347\n",
      "Validation loss: 14.2251, Validation accuracy: 0.9094\n",
      "\n",
      "Epoch 124: with learning rate 2.384185791015625e-08\n",
      "Training loss: 12.0738, Training accuracy: 0.9354\n",
      "Validation loss: 14.0241, Validation accuracy: 0.9111\n",
      "\n",
      "Epoch 125: with learning rate 2.384185791015625e-08\n",
      "Training loss: 12.0441, Training accuracy: 0.9357\n",
      "Validation loss: 14.0627, Validation accuracy: 0.9109\n",
      "\n",
      "Epoch 126: with learning rate 2.384185791015625e-08\n",
      "Training loss: 12.0428, Training accuracy: 0.9362\n",
      "Validation loss: 14.0646, Validation accuracy: 0.9117\n",
      "Learning rate decayed to 5.960464477539063e-09\n",
      "Saving ...\n",
      "\n",
      "Epoch 127: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0726, Training accuracy: 0.9355\n",
      "Validation loss: 14.1920, Validation accuracy: 0.9089\n",
      "\n",
      "Epoch 128: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.8147, Training accuracy: 0.9372\n",
      "Validation loss: 14.2541, Validation accuracy: 0.9096\n",
      "\n",
      "Epoch 129: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9697, Training accuracy: 0.9366\n",
      "Validation loss: 14.1537, Validation accuracy: 0.9111\n",
      "\n",
      "Epoch 130: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1365, Training accuracy: 0.9354\n",
      "Validation loss: 14.0605, Validation accuracy: 0.9112\n",
      "\n",
      "Epoch 131: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9773, Training accuracy: 0.9348\n",
      "Validation loss: 14.1737, Validation accuracy: 0.9100\n",
      "\n",
      "Epoch 132: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1258, Training accuracy: 0.9351\n",
      "Validation loss: 14.2155, Validation accuracy: 0.9087\n",
      "\n",
      "Epoch 133: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0308, Training accuracy: 0.9357\n",
      "Validation loss: 14.1427, Validation accuracy: 0.9091\n",
      "\n",
      "Epoch 134: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9815, Training accuracy: 0.9365\n",
      "Validation loss: 14.2111, Validation accuracy: 0.9097\n",
      "\n",
      "Epoch 135: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0562, Training accuracy: 0.9350\n",
      "Validation loss: 14.2312, Validation accuracy: 0.9088\n",
      "\n",
      "Epoch 136: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0926, Training accuracy: 0.9359\n",
      "Validation loss: 14.0973, Validation accuracy: 0.9091\n",
      "\n",
      "Epoch 137: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9387, Training accuracy: 0.9350\n",
      "Validation loss: 14.2004, Validation accuracy: 0.9098\n",
      "\n",
      "Epoch 138: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0190, Training accuracy: 0.9358\n",
      "Validation loss: 14.0573, Validation accuracy: 0.9103\n",
      "\n",
      "Epoch 139: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1497, Training accuracy: 0.9344\n",
      "Validation loss: 14.1874, Validation accuracy: 0.9085\n",
      "\n",
      "Epoch 140: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.8459, Training accuracy: 0.9357\n",
      "Validation loss: 14.1764, Validation accuracy: 0.9098\n",
      "\n",
      "Epoch 141: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0988, Training accuracy: 0.9356\n",
      "Validation loss: 14.3748, Validation accuracy: 0.9078\n",
      "\n",
      "Epoch 142: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9688, Training accuracy: 0.9354\n",
      "Validation loss: 14.1688, Validation accuracy: 0.9087\n",
      "\n",
      "Epoch 143: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9204, Training accuracy: 0.9364\n",
      "Validation loss: 14.2844, Validation accuracy: 0.9085\n",
      "\n",
      "Epoch 144: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0281, Training accuracy: 0.9363\n",
      "Validation loss: 14.2068, Validation accuracy: 0.9085\n",
      "\n",
      "Epoch 145: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1692, Training accuracy: 0.9351\n",
      "Validation loss: 14.1712, Validation accuracy: 0.9099\n",
      "\n",
      "Epoch 146: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0799, Training accuracy: 0.9352\n",
      "Validation loss: 14.0573, Validation accuracy: 0.9117\n",
      "\n",
      "Epoch 147: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0891, Training accuracy: 0.9346\n",
      "Validation loss: 14.0751, Validation accuracy: 0.9108\n",
      "\n",
      "Epoch 148: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9902, Training accuracy: 0.9354\n",
      "Validation loss: 14.0395, Validation accuracy: 0.9102\n",
      "\n",
      "Epoch 149: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0643, Training accuracy: 0.9344\n",
      "Validation loss: 14.0323, Validation accuracy: 0.9110\n",
      "\n",
      "Epoch 150: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9672, Training accuracy: 0.9364\n",
      "Validation loss: 14.2425, Validation accuracy: 0.9102\n",
      "\n",
      "Epoch 151: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0613, Training accuracy: 0.9359\n",
      "Validation loss: 14.2907, Validation accuracy: 0.9088\n",
      "\n",
      "Epoch 152: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0165, Training accuracy: 0.9365\n",
      "Validation loss: 14.2401, Validation accuracy: 0.9077\n",
      "\n",
      "Epoch 153: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9276, Training accuracy: 0.9360\n",
      "Validation loss: 14.1990, Validation accuracy: 0.9081\n",
      "\n",
      "Epoch 154: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1423, Training accuracy: 0.9355\n",
      "Validation loss: 14.2010, Validation accuracy: 0.9080\n",
      "\n",
      "Epoch 155: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1236, Training accuracy: 0.9348\n",
      "Validation loss: 14.0498, Validation accuracy: 0.9100\n",
      "\n",
      "Epoch 156: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0537, Training accuracy: 0.9367\n",
      "Validation loss: 14.1559, Validation accuracy: 0.9083\n",
      "\n",
      "Epoch 157: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0547, Training accuracy: 0.9359\n",
      "Validation loss: 14.0947, Validation accuracy: 0.9095\n",
      "\n",
      "Epoch 158: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1027, Training accuracy: 0.9363\n",
      "Validation loss: 14.2772, Validation accuracy: 0.9095\n",
      "\n",
      "Epoch 159: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0103, Training accuracy: 0.9359\n",
      "Validation loss: 14.0206, Validation accuracy: 0.9112\n",
      "\n",
      "Epoch 160: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9044, Training accuracy: 0.9362\n",
      "Validation loss: 14.0826, Validation accuracy: 0.9094\n",
      "\n",
      "Epoch 161: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.8121, Training accuracy: 0.9359\n",
      "Validation loss: 14.2905, Validation accuracy: 0.9079\n",
      "\n",
      "Epoch 162: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0944, Training accuracy: 0.9358\n",
      "Validation loss: 14.1395, Validation accuracy: 0.9094\n",
      "\n",
      "Epoch 163: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1257, Training accuracy: 0.9353\n",
      "Validation loss: 14.0462, Validation accuracy: 0.9107\n",
      "\n",
      "Epoch 164: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0427, Training accuracy: 0.9361\n",
      "Validation loss: 14.1887, Validation accuracy: 0.9092\n",
      "\n",
      "Epoch 165: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1778, Training accuracy: 0.9335\n",
      "Validation loss: 14.1812, Validation accuracy: 0.9095\n",
      "\n",
      "Epoch 166: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1957, Training accuracy: 0.9337\n",
      "Validation loss: 14.1256, Validation accuracy: 0.9094\n",
      "\n",
      "Epoch 167: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9989, Training accuracy: 0.9370\n",
      "Validation loss: 14.1208, Validation accuracy: 0.9093\n",
      "\n",
      "Epoch 168: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0127, Training accuracy: 0.9357\n",
      "Validation loss: 14.2663, Validation accuracy: 0.9104\n",
      "\n",
      "Epoch 169: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0647, Training accuracy: 0.9360\n",
      "Validation loss: 14.1674, Validation accuracy: 0.9102\n",
      "\n",
      "Epoch 170: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1262, Training accuracy: 0.9354\n",
      "Validation loss: 14.2675, Validation accuracy: 0.9075\n",
      "\n",
      "Epoch 171: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9864, Training accuracy: 0.9359\n",
      "Validation loss: 14.2379, Validation accuracy: 0.9110\n",
      "\n",
      "Epoch 172: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0709, Training accuracy: 0.9356\n",
      "Validation loss: 14.0598, Validation accuracy: 0.9091\n",
      "\n",
      "Epoch 173: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.2211, Training accuracy: 0.9338\n",
      "Validation loss: 14.1758, Validation accuracy: 0.9101\n",
      "\n",
      "Epoch 174: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9988, Training accuracy: 0.9355\n",
      "Validation loss: 14.0634, Validation accuracy: 0.9102\n",
      "\n",
      "Epoch 175: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9567, Training accuracy: 0.9357\n",
      "Validation loss: 14.1049, Validation accuracy: 0.9085\n",
      "\n",
      "Epoch 176: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0386, Training accuracy: 0.9354\n",
      "Validation loss: 14.1654, Validation accuracy: 0.9087\n",
      "\n",
      "Epoch 177: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1847, Training accuracy: 0.9349\n",
      "Validation loss: 14.2072, Validation accuracy: 0.9099\n",
      "\n",
      "Epoch 178: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.8722, Training accuracy: 0.9357\n",
      "Validation loss: 14.1892, Validation accuracy: 0.9095\n",
      "\n",
      "Epoch 179: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1040, Training accuracy: 0.9346\n",
      "Validation loss: 14.1459, Validation accuracy: 0.9102\n",
      "\n",
      "Epoch 180: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1003, Training accuracy: 0.9357\n",
      "Validation loss: 14.2157, Validation accuracy: 0.9077\n",
      "\n",
      "Epoch 181: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1099, Training accuracy: 0.9344\n",
      "Validation loss: 14.1750, Validation accuracy: 0.9105\n",
      "\n",
      "Epoch 182: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0809, Training accuracy: 0.9345\n",
      "Validation loss: 14.1923, Validation accuracy: 0.9095\n",
      "\n",
      "Epoch 183: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.1077, Training accuracy: 0.9349\n",
      "Validation loss: 14.2650, Validation accuracy: 0.9074\n",
      "\n",
      "Epoch 184: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0919, Training accuracy: 0.9349\n",
      "Validation loss: 14.1062, Validation accuracy: 0.9105\n",
      "\n",
      "Epoch 185: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9932, Training accuracy: 0.9356\n",
      "Validation loss: 14.1664, Validation accuracy: 0.9092\n",
      "\n",
      "Epoch 186: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9838, Training accuracy: 0.9364\n",
      "Validation loss: 14.1132, Validation accuracy: 0.9109\n",
      "\n",
      "Epoch 187: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0040, Training accuracy: 0.9366\n",
      "Validation loss: 14.1021, Validation accuracy: 0.9092\n",
      "\n",
      "Epoch 188: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0057, Training accuracy: 0.9362\n",
      "Validation loss: 14.2188, Validation accuracy: 0.9087\n",
      "\n",
      "Epoch 189: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9160, Training accuracy: 0.9354\n",
      "Validation loss: 14.1459, Validation accuracy: 0.9090\n",
      "\n",
      "Epoch 190: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.8951, Training accuracy: 0.9367\n",
      "Validation loss: 14.2078, Validation accuracy: 0.9080\n",
      "\n",
      "Epoch 191: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.2017, Training accuracy: 0.9360\n",
      "Validation loss: 14.0061, Validation accuracy: 0.9106\n",
      "\n",
      "Epoch 192: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0193, Training accuracy: 0.9359\n",
      "Validation loss: 14.0959, Validation accuracy: 0.9108\n",
      "\n",
      "Epoch 193: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0953, Training accuracy: 0.9350\n",
      "Validation loss: 14.0381, Validation accuracy: 0.9110\n",
      "\n",
      "Epoch 194: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.7825, Training accuracy: 0.9376\n",
      "Validation loss: 14.0510, Validation accuracy: 0.9090\n",
      "\n",
      "Epoch 195: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9719, Training accuracy: 0.9349\n",
      "Validation loss: 14.0887, Validation accuracy: 0.9098\n",
      "\n",
      "Epoch 196: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9982, Training accuracy: 0.9360\n",
      "Validation loss: 14.2072, Validation accuracy: 0.9081\n",
      "\n",
      "Epoch 197: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9974, Training accuracy: 0.9361\n",
      "Validation loss: 14.1308, Validation accuracy: 0.9100\n",
      "\n",
      "Epoch 198: with learning rate 5.960464477539063e-09\n",
      "Training loss: 11.9893, Training accuracy: 0.9363\n",
      "Validation loss: 14.1652, Validation accuracy: 0.9099\n",
      "\n",
      "Epoch 199: with learning rate 5.960464477539063e-09\n",
      "Training loss: 12.0154, Training accuracy: 0.9355\n",
      "Validation loss: 14.2018, Validation accuracy: 0.9096\n",
      "\n",
      "==================================================\n",
      "==> Optimization finished! Best validation accuracy: 0.9117\n"
     ]
    }
   ],
   "source": [
    "# ===== Import necessary libraries =====\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# ===== Set up the SimpleNN model =====\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion*out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion*out_channels, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion*out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, architecture_type='cnn', conv_layer_configs=None, fc_layer_configs=None, res_block_configs=None, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Initialization code for other parts of the class remains the same\n",
    "        \n",
    "        if architecture_type == 'cnn':\n",
    "            self.myNetworkType = 'cnn'\n",
    "            self.features = self._make_cnn_layers(conv_layer_configs)\n",
    "            prev_features = conv_layer_configs[-1]['out_channels']\n",
    "        elif architecture_type == 'resnet':\n",
    "            self.myNetworkType = 'resnet'\n",
    "            self.init_conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.init_bn = nn.BatchNorm2d(16)\n",
    "            self.init_relu = nn.ReLU(inplace=True)\n",
    "            self.features, prev_features = self._make_resnet_layers(res_block_configs)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type: {}\".format(architecture_type))\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = self._make_fc_layers(prev_features, fc_layer_configs, num_classes)\n",
    "\n",
    "    def _make_cnn_layers(self, conv_layer_configs):\n",
    "        layers = []\n",
    "        for conv_layer in conv_layer_configs:\n",
    "            in_channels = conv_layer['in_channels']\n",
    "            out_channels = conv_layer['out_channels']\n",
    "            kernel_size = conv_layer['kernel_size']\n",
    "            stride = conv_layer['stride']\n",
    "            padding = conv_layer['padding']\n",
    "            block = CNNBlock(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            layers.append(block)\n",
    "        in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _make_resnet_layers(self, res_block_configs):\n",
    "        layers = []\n",
    "        for block_config in res_block_configs:\n",
    "            in_channels = block_config['in_channels']\n",
    "            out_channels = block_config['out_channels']\n",
    "            num_blocks = block_config['num_blocks']\n",
    "            stride = block_config['stride']\n",
    "            \n",
    "            layers.append(self._make_layer(ResBlock, in_channels, out_channels, num_blocks, stride))\n",
    "            \n",
    "            # Update in_channels for the next set of blocks\n",
    "            in_channels = out_channels * ResBlock.expansion\n",
    "        return nn.Sequential(*layers), in_channels\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)  # First block might have a stride to downsample\n",
    "        blocks = []\n",
    "        for stride in strides:\n",
    "            blocks.append(block(in_channels, out_channels, stride))\n",
    "            in_channels = out_channels * block.expansion  # Update in_channels for the next block\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def _make_fc_layers(self, prev_features, fc_layer_configs, num_classes):\n",
    "        layers = nn.ModuleList()\n",
    "        if fc_layer_configs is not None:\n",
    "            for fc_layer in fc_layer_configs:\n",
    "                layers.append(nn.Linear(prev_features, fc_layer['out_features']))\n",
    "                prev_features = fc_layer['out_features']\n",
    "            layers.append(nn.Linear(prev_features, num_classes))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ===== Define and set HyperParameters =====\n",
    "\n",
    "\n",
    "## DataLoader\n",
    "TRAIN_BATCH_SIZE = 64  # training batch size\n",
    "VAL_BATCH_SIZE = 50  # validation batch size\n",
    "NUM_WORKERS = 8  # number of workers for DataLoader\n",
    "\n",
    "## Model\n",
    "### CNN\n",
    "conv_layer_configs = [\n",
    "    {'in_channels': 3, 'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'in_channels': 32, 'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'in_channels': 64, 'out_channels': 128, 'kernel_size': 3, 'stride': 2, 'padding': 1}\n",
    "]\n",
    "\n",
    "fc_layer_configs = [\n",
    "    {'out_features': 256},\n",
    "    {'out_features': 128}\n",
    "]\n",
    "\n",
    "### ResNet\n",
    "res_block_configs = [\n",
    "    {'in_channels': 3, 'out_channels': 16, 'stride': 1, 'num_blocks': 3},\n",
    "    {'in_channels': 16, 'out_channels': 32, 'stride': 2, 'num_blocks': 3},\n",
    "    {'in_channels': 32, 'out_channels': 64, 'stride': 2, 'num_blocks': 3}\n",
    "]\n",
    "\n",
    "## Optimizer and scheduler\n",
    "INITIAL_LR = 0.1  # initial learning rate\n",
    "MOMENTUM = 0.9  # momentum for optimizer\n",
    "REG = 1e-4  # L2 regularization strength\n",
    "LR_PATIENCE = 5  # Patience for ReduceLROnPlateau scheduler\n",
    "LR_FACTOR = 0.25  # Factor by which the learning rate will be reduced\n",
    "\n",
    "## Training\n",
    "EPOCHS = 200  # total number of training epochs\n",
    "CHECKPOINT_FOLDER = \"./saved_models\"  # folder where models are saved\n",
    "\n",
    "\n",
    "# ===== Set up preprocessing functions =====\n",
    "\n",
    "\n",
    "## specify preprocessing function\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "\n",
    "# ===== Set up dataset and dataloader =====\n",
    "\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "\n",
    "## construct dataset\n",
    "train_set = CIFAR10(\n",
    "    root = DATA_ROOT, \n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform_train\n",
    ")\n",
    "val_set = CIFAR10(\n",
    "    root = DATA_ROOT, \n",
    "    train = False, \n",
    "    download = True,\n",
    "    transform = transform_val\n",
    ")\n",
    "\n",
    "## construct dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size= TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# ===== Instantiate your SimpleNN model and deploy it to device =====\n",
    "\n",
    "\n",
    "## specify the device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# For CNN\n",
    "# net = CNN(architecture_type='cnn', conv_layer_configs=conv_layer_configs, fc_layer_configs=fc_layer_configs, num_classes=10)\n",
    "# For ResNet\n",
    "net = CNN(architecture_type='resnet', res_block_configs=res_block_configs, num_classes=10)\n",
    "# deploy the network to device\n",
    "net.to(device)\n",
    "\n",
    "print(next(net.parameters()).device)\n",
    "\n",
    "\n",
    "# ===== Set up the loss function and optimizer =====\n",
    "\n",
    "\n",
    "## loss function\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "## Add optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=LR_PATIENCE, factor=LR_FACTOR)\n",
    "\n",
    "\n",
    "# ===== Start the training process =====\n",
    "\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"==> Training starts!\")\n",
    "print(\"=\"*50)\n",
    "for i in range(0, EPOCHS):    \n",
    "    ## switch to train mode\n",
    "    net.train()\n",
    "\n",
    "    ## print the Epoch and learning rate\n",
    "    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {i}: with learning rate {current_learning_rate}\")\n",
    "    \n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    ## Train the model\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        ### copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        ### compute the output and loss\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets.long())\n",
    "        \n",
    "        ### zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ### apply gradient and update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### count the number of correctly predicted samples in the current batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_examples += targets.size(0)\n",
    "        correct_examples += (predicted == targets).sum().item()\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "                \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "\n",
    "    ## Validate on the validation dataset\n",
    "    ## switch to eval mode\n",
    "    net.eval()\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    ## disable gradient during validation, which can save GPU memory\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            ### copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            ### compute the output and loss\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets.long())\n",
    "            \n",
    "            ### count the number of correctly predicted samples in the current batch\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_examples += targets.size(0)\n",
    "            correct_examples += (predicted == targets).sum().item()\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "\n",
    "    ## decay learning rate\n",
    "    previous_learning_rate = current_learning_rate\n",
    "    scheduler.step(val_loss)\n",
    "    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "    if previous_learning_rate != current_learning_rate:\n",
    "        print(f\"Learning rate decayed to {current_learning_rate}\")\n",
    "    \n",
    "    ## save the model checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "           os.makedirs(CHECKPOINT_FOLDER)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'state_dict': net.state_dict(),\n",
    "                'epoch': i,\n",
    "                'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'CNN.pth'))\n",
    "        \n",
    "    print('')\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
