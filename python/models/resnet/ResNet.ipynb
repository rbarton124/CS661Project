{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ===== Import necessary libraries =====\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch.quantization\n",
    "from torch.quantization import QConfig, default_observer, default_per_channel_weight_observer\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# ===== Set up the SimpleNN model =====\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, quantTrue=False):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def prune_weights(self, amount):\n",
    "        prune.l1_unstructured(self.conv, 'weight', amount=amount)\n",
    "\n",
    "    def to_sparse(self):\n",
    "        self.conv.weight = nn.Parameter(self.conv.weight.to_sparse())\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, quantTrue=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion*out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion*out_channels, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion*out_channels)\n",
    "            )\n",
    "        \n",
    "        self.quantadd = nn.quantized.FloatFunctional()\n",
    "\n",
    "        self.quantTrue = quantTrue\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.quantTrue:\n",
    "            if self.shortcut is not None:\n",
    "                identity = self.shortcut(identity)\n",
    "            out = self.quantadd.add(out, identity)\n",
    "        else:\n",
    "            if self.shortcut is not None:\n",
    "                identity = self.shortcut(identity)\n",
    "            out += identity\n",
    "\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "    def prune_weights(self, amount):\n",
    "        prune.l1_unstructured(self.conv1, 'weight', amount=amount)\n",
    "        prune.l1_unstructured(self.conv2, 'weight', amount=amount)\n",
    "\n",
    "    def to_sparse(self):\n",
    "        self.conv1.weight = nn.Parameter(self.conv1.weight.to_sparse())\n",
    "        self.conv2.weight = nn.Parameter(self.conv2.weight.to_sparse())\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, architecture_type='cnn', quantTrue = False, conv_layer_configs=None, fc_layer_configs=None, res_block_configs=None, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Initialization code for other parts of the class remains the same\n",
    "        \n",
    "        if architecture_type == 'cnn':\n",
    "            self.myNetworkType = 'cnn'\n",
    "            self.features = self._make_cnn_layers(conv_layer_configs)\n",
    "            prev_features = conv_layer_configs[-1]['out_channels']\n",
    "        elif architecture_type == 'resnet':\n",
    "            self.myNetworkType = 'resnet'\n",
    "            self.init_conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.init_bn = nn.BatchNorm2d(16)\n",
    "            self.init_relu = nn.ReLU(inplace=True)\n",
    "            self.features, prev_features = self._make_resnet_layers(res_block_configs, quantTrue)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type: {}\".format(architecture_type))\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = self._make_fc_layers(prev_features, fc_layer_configs, num_classes)\n",
    "\n",
    "    def _make_cnn_layers(self, conv_layer_configs):\n",
    "        layers = []\n",
    "        for conv_layer in conv_layer_configs:\n",
    "            in_channels = conv_layer['in_channels']\n",
    "            out_channels = conv_layer['out_channels']\n",
    "            kernel_size = conv_layer['kernel_size']\n",
    "            stride = conv_layer['stride']\n",
    "            padding = conv_layer['padding']\n",
    "            block = CNNBlock(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            layers.append(block)\n",
    "        in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _make_resnet_layers(self, res_block_configs, quantTrue=False):\n",
    "        layers = []\n",
    "        for block_config in res_block_configs:\n",
    "            in_channels = block_config['in_channels']\n",
    "            out_channels = block_config['out_channels']\n",
    "            num_blocks = block_config['num_blocks']\n",
    "            stride = block_config['stride']\n",
    "            \n",
    "            layers.append(self._make_layer(ResBlock, in_channels, out_channels, num_blocks, stride, quantTrue=quantTrue))\n",
    "            \n",
    "            # Update in_channels for the next set of blocks\n",
    "            in_channels = out_channels * ResBlock.expansion\n",
    "        return nn.Sequential(*layers), in_channels\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride, quantTrue = False):\n",
    "        strides = [stride] + [1]*(num_blocks-1)  # First block might have a stride to downsample\n",
    "        blocks = []\n",
    "        for stride in strides:\n",
    "            blocks.append(block(in_channels, out_channels, stride, quantTrue=quantTrue))\n",
    "            in_channels = out_channels * block.expansion  # Update in_channels for the next block\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def _make_fc_layers(self, prev_features, fc_layer_configs, num_classes):\n",
    "        layers = nn.ModuleList()\n",
    "        if fc_layer_configs is not None:\n",
    "            for fc_layer in fc_layer_configs:\n",
    "                layers.append(nn.Linear(prev_features, fc_layer['out_features']))\n",
    "                prev_features = fc_layer['out_features']\n",
    "            layers.append(nn.Linear(prev_features, num_classes))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class QuantizablePrunableCNN(CNN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(QuantizablePrunableCNN, self).__init__(*args, **kwargs)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "    def prune_weights(self, amount=0.15):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (CNNBlock, ResBlock)):\n",
    "                module.prune_weights(amount)\n",
    "\n",
    "    def convert_to_sparse(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (CNNBlock, ResBlock)):\n",
    "                module.to_sparse()\n",
    "\n",
    "\n",
    "\n",
    "# ===== Define and set HyperParameters =====\n",
    "\n",
    "\n",
    "## DataLoader\n",
    "TRAIN_BATCH_SIZE = 64  # training batch size\n",
    "VAL_BATCH_SIZE = 50  # validation batch size\n",
    "NUM_WORKERS = 8  # number of workers for DataLoader\n",
    "\n",
    "## Model\n",
    "### CNN\n",
    "conv_layer_configs = [\n",
    "    {'in_channels': 3, 'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'in_channels': 32, 'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'in_channels': 64, 'out_channels': 128, 'kernel_size': 3, 'stride': 2, 'padding': 1}\n",
    "]\n",
    "\n",
    "fc_layer_configs = [\n",
    "    {'out_features': 256},\n",
    "    {'out_features': 128}\n",
    "]\n",
    "\n",
    "### ResNet\n",
    "res_block_configs = [\n",
    "    {'in_channels': 3, 'out_channels': 16, 'stride': 1, 'num_blocks': 3},\n",
    "    {'in_channels': 16, 'out_channels': 32, 'stride': 2, 'num_blocks': 3},\n",
    "    {'in_channels': 32, 'out_channels': 64, 'stride': 2, 'num_blocks': 3}\n",
    "]\n",
    "\n",
    "## Optimizer and scheduler\n",
    "INITIAL_LR = 0.1  # initial learning rate\n",
    "MOMENTUM = 0.9  # momentum for optimizer\n",
    "REG = 1e-4  # L2 regularization strength\n",
    "LR_PATIENCE = 5  # Patience for ReduceLROnPlateau scheduler\n",
    "LR_FACTOR = 0.25  # Factor by which the learning rate will be reduced\n",
    "\n",
    "## Training\n",
    "EPOCHS = 3  # total number of training epochs\n",
    "CHECKPOINT_FOLDER = \"./saved_models\"  # folder where models are saved\n",
    "\n",
    "## Pruning and Quantization\n",
    "ENABLE_QUANTIZATION = False\n",
    "ENABLE_PRUNING = True\n",
    "\n",
    "# ===== Set up preprocessing functions =====\n",
    "\n",
    "\n",
    "## specify preprocessing function\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "\n",
    "# ===== Set up dataset and dataloader =====\n",
    "\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "\n",
    "## construct dataset\n",
    "train_set = CIFAR10(\n",
    "    root = DATA_ROOT, \n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform_train\n",
    ")\n",
    "val_set = CIFAR10(\n",
    "    root = DATA_ROOT, \n",
    "    train = False, \n",
    "    download = True,\n",
    "    transform = transform_val\n",
    ")\n",
    "\n",
    "## construct dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size= TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# ===== Instantiate your SimpleNN model and deploy it to device =====\n",
    "\n",
    "\n",
    "## specify the device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = QuantizablePrunableCNN(architecture_type='resnet', quantTrue=ENABLE_QUANTIZATION, res_block_configs=res_block_configs, num_classes=10) \n",
    "# net = QuantizableCNN(architecture_type='cnn', quantTrue=ENABLE_QUANTIZATION, conv_layer_configs=conv_layer_configs, fc_layer_configs=fc_layer_configs, num_classes=10)\n",
    "\n",
    "\n",
    "if ENABLE_QUANTIZATION:\n",
    "    my_qconfig = QConfig(\n",
    "    activation=torch.quantization.default_observer.with_args(dtype=torch.quint8),\n",
    "    weight=torch.quantization.default_weight_observer.with_args(dtype=torch.qint8)\n",
    "    )\n",
    "    net.qconfig = my_qconfig\n",
    "    torch.quantization.prepare_qat(net, inplace=True)\n",
    "\n",
    "# deploy the network to device\n",
    "net.to(device)\n",
    "\n",
    "print(next(net.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training starts!\n",
      "==================================================\n",
      "Epoch 0: with learning rate 0.1\n",
      "Training loss: 109.1854, Training accuracy: 0.3772\n",
      "Applying pruning: 10.00%\n",
      "Validation loss: 74.1171, Validation accuracy: 0.4678\n",
      "Saving ...\n",
      "\n",
      "Epoch 1: with learning rate 0.1\n",
      "Training loss: 81.6675, Training accuracy: 0.5395\n",
      "Applying pruning: 10.13%\n",
      "Validation loss: 60.7095, Validation accuracy: 0.5866\n",
      "Saving ...\n",
      "\n",
      "Epoch 2: with learning rate 0.1\n",
      "Training loss: 69.5525, Training accuracy: 0.6120\n",
      "Applying pruning: 10.25%\n",
      "Validation loss: 54.7973, Validation accuracy: 0.6203\n",
      "Saving ...\n",
      "\n",
      "Epoch 3: with learning rate 0.1\n",
      "Training loss: 62.9852, Training accuracy: 0.6528\n",
      "Applying pruning: 10.38%\n",
      "Validation loss: 50.1403, Validation accuracy: 0.6549\n",
      "Saving ...\n",
      "\n",
      "Epoch 4: with learning rate 0.1\n",
      "Training loss: 57.3567, Training accuracy: 0.6836\n",
      "Applying pruning: 10.50%\n",
      "Validation loss: 45.2189, Validation accuracy: 0.6921\n",
      "Saving ...\n",
      "\n",
      "Epoch 5: with learning rate 0.1\n",
      "Training loss: 52.3066, Training accuracy: 0.7153\n",
      "Applying pruning: 10.63%\n",
      "Validation loss: 43.5934, Validation accuracy: 0.7082\n",
      "Saving ...\n",
      "\n",
      "Epoch 6: with learning rate 0.1\n",
      "Training loss: 49.9418, Training accuracy: 0.7257\n",
      "Applying pruning: 10.75%\n",
      "Validation loss: 38.5641, Validation accuracy: 0.7423\n",
      "Saving ...\n",
      "\n",
      "Epoch 7: with learning rate 0.1\n",
      "Training loss: 47.1696, Training accuracy: 0.7422\n",
      "Applying pruning: 10.88%\n",
      "Validation loss: 34.2556, Validation accuracy: 0.7603\n",
      "Saving ...\n",
      "\n",
      "Epoch 8: with learning rate 0.1\n",
      "Training loss: 45.4894, Training accuracy: 0.7533\n",
      "Applying pruning: 11.01%\n",
      "Validation loss: 39.8057, Validation accuracy: 0.7272\n",
      "\n",
      "Epoch 9: with learning rate 0.1\n",
      "Training loss: 43.9569, Training accuracy: 0.7615\n",
      "Applying pruning: 11.13%\n",
      "Validation loss: 35.2894, Validation accuracy: 0.7640\n",
      "Saving ...\n",
      "\n",
      "Epoch 10: with learning rate 0.1\n",
      "Training loss: 42.5052, Training accuracy: 0.7695\n",
      "Applying pruning: 11.26%\n",
      "Validation loss: 37.8606, Validation accuracy: 0.7514\n",
      "\n",
      "Epoch 11: with learning rate 0.1\n",
      "Training loss: 41.6003, Training accuracy: 0.7734\n",
      "Applying pruning: 11.38%\n",
      "Validation loss: 44.9943, Validation accuracy: 0.7019\n",
      "\n",
      "Epoch 12: with learning rate 0.1\n",
      "Training loss: 40.8625, Training accuracy: 0.7792\n",
      "Applying pruning: 11.51%\n",
      "Validation loss: 42.3450, Validation accuracy: 0.7313\n",
      "\n",
      "Epoch 13: with learning rate 0.1\n",
      "Training loss: 40.2863, Training accuracy: 0.7793\n",
      "Applying pruning: 11.63%\n",
      "Validation loss: 41.7517, Validation accuracy: 0.7253\n",
      "Learning rate decayed to 0.025\n",
      "\n",
      "Epoch 14: with learning rate 0.025\n",
      "Training loss: 35.0511, Training accuracy: 0.8096\n",
      "Applying pruning: 11.76%\n",
      "Validation loss: 35.7089, Validation accuracy: 0.7654\n",
      "Saving ...\n",
      "\n",
      "Epoch 15: with learning rate 0.025\n",
      "Training loss: 33.7353, Training accuracy: 0.8174\n",
      "Applying pruning: 11.88%\n",
      "Validation loss: 86.9316, Validation accuracy: 0.5311\n",
      "\n",
      "Epoch 16: with learning rate 0.025\n",
      "Training loss: 34.0760, Training accuracy: 0.8146\n",
      "Applying pruning: 12.01%\n",
      "Validation loss: 45.0630, Validation accuracy: 0.6997\n",
      "\n",
      "Epoch 17: with learning rate 0.025\n",
      "Training loss: 34.3611, Training accuracy: 0.8136\n",
      "Applying pruning: 12.14%\n",
      "Validation loss: 72.0895, Validation accuracy: 0.5903\n",
      "\n",
      "Epoch 18: with learning rate 0.025\n",
      "Training loss: 34.4440, Training accuracy: 0.8123\n",
      "Applying pruning: 12.26%\n",
      "Validation loss: 46.5106, Validation accuracy: 0.6901\n",
      "\n",
      "Epoch 19: with learning rate 0.025\n",
      "Training loss: 35.0466, Training accuracy: 0.8086\n",
      "Applying pruning: 12.39%\n",
      "Validation loss: 53.6495, Validation accuracy: 0.6452\n",
      "Learning rate decayed to 0.00625\n",
      "\n",
      "Epoch 20: with learning rate 0.00625\n",
      "Training loss: 35.0401, Training accuracy: 0.8106\n",
      "Applying pruning: 12.51%\n",
      "Validation loss: 108.8207, Validation accuracy: 0.4060\n",
      "\n",
      "Epoch 21: with learning rate 0.00625\n",
      "Training loss: 36.0362, Training accuracy: 0.8056\n",
      "Applying pruning: 12.64%\n",
      "Validation loss: 84.7201, Validation accuracy: 0.5478\n",
      "\n",
      "Epoch 22: with learning rate 0.00625\n",
      "Training loss: 37.8183, Training accuracy: 0.7943\n",
      "Applying pruning: 12.76%\n",
      "Validation loss: 52.2806, Validation accuracy: 0.6459\n",
      "\n",
      "Epoch 23: with learning rate 0.00625\n",
      "Training loss: 39.8856, Training accuracy: 0.7810\n",
      "Applying pruning: 12.89%\n",
      "Validation loss: 107.3486, Validation accuracy: 0.4524\n",
      "\n",
      "Epoch 24: with learning rate 0.00625\n",
      "Training loss: 41.4591, Training accuracy: 0.7769\n",
      "Applying pruning: 13.02%\n",
      "Validation loss: 129.8521, Validation accuracy: 0.3468\n",
      "\n",
      "Epoch 25: with learning rate 0.00625\n",
      "Training loss: 43.4834, Training accuracy: 0.7612\n",
      "Applying pruning: 13.14%\n",
      "Validation loss: 164.5430, Validation accuracy: 0.2929\n",
      "Learning rate decayed to 0.0015625\n",
      "\n",
      "Epoch 26: with learning rate 0.0015625\n",
      "Training loss: 47.4368, Training accuracy: 0.7414\n",
      "Applying pruning: 13.27%\n",
      "Validation loss: 331.1892, Validation accuracy: 0.2674\n",
      "\n",
      "Epoch 27: with learning rate 0.0015625\n",
      "Training loss: 51.7531, Training accuracy: 0.7174\n",
      "Applying pruning: 13.39%\n",
      "Validation loss: 125.3302, Validation accuracy: 0.3747\n",
      "\n",
      "Epoch 28: with learning rate 0.0015625\n",
      "Training loss: 55.3067, Training accuracy: 0.6996\n",
      "Applying pruning: 13.52%\n",
      "Validation loss: 133.7582, Validation accuracy: 0.2896\n",
      "\n",
      "Epoch 29: with learning rate 0.0015625\n",
      "Training loss: 60.2883, Training accuracy: 0.6742\n",
      "Applying pruning: 13.64%\n",
      "Validation loss: 442.0164, Validation accuracy: 0.1413\n",
      "\n",
      "Epoch 30: with learning rate 0.0015625\n",
      "Training loss: 66.5950, Training accuracy: 0.6377\n",
      "Applying pruning: 13.77%\n",
      "Validation loss: 139.0576, Validation accuracy: 0.1840\n",
      "\n",
      "Epoch 31: with learning rate 0.0015625\n",
      "Training loss: 68.5752, Training accuracy: 0.6248\n",
      "Applying pruning: 13.89%\n",
      "Validation loss: 140.8530, Validation accuracy: 0.2866\n",
      "Learning rate decayed to 0.000390625\n",
      "\n",
      "Epoch 32: with learning rate 0.000390625\n",
      "Training loss: 81.3843, Training accuracy: 0.5554\n",
      "Applying pruning: 14.02%\n",
      "Validation loss: 145.1630, Validation accuracy: 0.2742\n",
      "\n",
      "Epoch 33: with learning rate 0.000390625\n",
      "Training loss: 81.2393, Training accuracy: 0.5543\n",
      "Applying pruning: 14.15%\n",
      "Validation loss: 134.7242, Validation accuracy: 0.2580\n",
      "\n",
      "Epoch 34: with learning rate 0.000390625\n",
      "Training loss: 89.7231, Training accuracy: 0.5027\n",
      "Applying pruning: 14.27%\n",
      "Validation loss: 132.4419, Validation accuracy: 0.2458\n",
      "\n",
      "Epoch 35: with learning rate 0.000390625\n",
      "Training loss: 90.7425, Training accuracy: 0.4979\n",
      "Applying pruning: 14.40%\n",
      "Validation loss: 263.3065, Validation accuracy: 0.1719\n",
      "\n",
      "Epoch 36: with learning rate 0.000390625\n",
      "Training loss: 97.0784, Training accuracy: 0.4609\n",
      "Applying pruning: 14.52%\n",
      "Validation loss: 224.4197, Validation accuracy: 0.2072\n",
      "\n",
      "Epoch 37: with learning rate 0.000390625\n",
      "Training loss: 102.3431, Training accuracy: 0.4324\n",
      "Applying pruning: 14.65%\n",
      "Validation loss: 8532.4981, Validation accuracy: 0.1000\n",
      "Learning rate decayed to 9.765625e-05\n",
      "\n",
      "Epoch 38: with learning rate 9.765625e-05\n",
      "Training loss: 112.8765, Training accuracy: 0.3728\n",
      "Applying pruning: 14.77%\n",
      "Validation loss: 356.8501, Validation accuracy: 0.1282\n",
      "\n",
      "Epoch 39: with learning rate 9.765625e-05\n",
      "Training loss: 122.5662, Training accuracy: 0.3216\n",
      "Applying pruning: 14.90%\n",
      "Validation loss: 596.5156, Validation accuracy: 0.1605\n",
      "\n",
      "Epoch 40: with learning rate 9.765625e-05\n",
      "Training loss: 128.0832, Training accuracy: 0.2982\n",
      "Applying pruning: 15.03%\n",
      "Validation loss: 1180.5378, Validation accuracy: 0.1462\n",
      "\n",
      "Epoch 41: with learning rate 9.765625e-05\n",
      "Training loss: 130.3885, Training accuracy: 0.2733\n",
      "Applying pruning: 15.15%\n",
      "Validation loss: 210.2816, Validation accuracy: 0.1298\n",
      "\n",
      "Epoch 42: with learning rate 9.765625e-05\n",
      "Training loss: 127.6480, Training accuracy: 0.2859\n",
      "Applying pruning: 15.28%\n",
      "Validation loss: 315.2419, Validation accuracy: 0.1444\n",
      "\n",
      "Epoch 43: with learning rate 9.765625e-05\n",
      "Training loss: 127.7109, Training accuracy: 0.2729\n",
      "Applying pruning: 15.40%\n",
      "Validation loss: 297.0825, Validation accuracy: 0.1342\n",
      "Learning rate decayed to 2.44140625e-05\n",
      "\n",
      "Epoch 44: with learning rate 2.44140625e-05\n",
      "Training loss: 130.8890, Training accuracy: 0.2510\n",
      "Applying pruning: 15.53%\n",
      "Validation loss: 198324.5173, Validation accuracy: 0.1000\n",
      "\n",
      "Epoch 45: with learning rate 2.44140625e-05\n",
      "Training loss: 143.5553, Training accuracy: 0.2234\n",
      "Applying pruning: 15.65%\n",
      "Validation loss: 5128.4489, Validation accuracy: 0.1000\n",
      "\n",
      "Epoch 46: with learning rate 2.44140625e-05\n",
      "Training loss: 147.7749, Training accuracy: 0.2098\n",
      "Applying pruning: 15.78%\n",
      "Validation loss: 176.2804, Validation accuracy: 0.1408\n",
      "\n",
      "Epoch 47: with learning rate 2.44140625e-05\n",
      "Training loss: 145.7207, Training accuracy: 0.1664\n",
      "Applying pruning: 15.90%\n",
      "Validation loss: 439.5871, Validation accuracy: 0.1005\n",
      "\n",
      "Epoch 48: with learning rate 2.44140625e-05\n",
      "Training loss: 137.9327, Training accuracy: 0.2109\n",
      "Applying pruning: 16.03%\n",
      "Validation loss: 1982.5126, Validation accuracy: 0.1000\n",
      "\n",
      "Epoch 49: with learning rate 2.44140625e-05\n",
      "Training loss: 144.3328, Training accuracy: 0.1908\n",
      "Applying pruning: 16.16%\n",
      "Validation loss: 143.6081, Validation accuracy: 0.1489\n",
      "Learning rate decayed to 6.103515625e-06\n",
      "\n",
      "Epoch 50: with learning rate 6.103515625e-06\n",
      "Training loss: 149.2838, Training accuracy: 0.1709\n",
      "Applying pruning: 16.28%\n",
      "Validation loss: 193.8959, Validation accuracy: 0.1101\n",
      "\n",
      "Epoch 51: with learning rate 6.103515625e-06\n",
      "Training loss: 148.1059, Training accuracy: 0.1693\n",
      "Applying pruning: 16.41%\n",
      "Validation loss: 245.1581, Validation accuracy: 0.1102\n",
      "\n",
      "Epoch 52: with learning rate 6.103515625e-06\n",
      "Training loss: 155.0760, Training accuracy: 0.1556\n",
      "Applying pruning: 16.53%\n",
      "Validation loss: 145.8233, Validation accuracy: 0.1550\n",
      "\n",
      "Epoch 53: with learning rate 6.103515625e-06\n",
      "Training loss: 157.7772, Training accuracy: 0.1309\n",
      "Applying pruning: 16.66%\n",
      "Validation loss: 312.4462, Validation accuracy: 0.1015\n",
      "\n",
      "Epoch 54: with learning rate 6.103515625e-06\n",
      "Training loss: 157.7115, Training accuracy: 0.1233\n",
      "Applying pruning: 16.78%\n",
      "Validation loss: 225.3407, Validation accuracy: 0.1442\n",
      "\n",
      "Epoch 55: with learning rate 6.103515625e-06\n",
      "Training loss: 151.5897, Training accuracy: 0.1431\n",
      "Applying pruning: 16.91%\n",
      "Validation loss: 121.4784, Validation accuracy: 0.1561\n",
      "Learning rate decayed to 1.52587890625e-06\n",
      "\n",
      "Epoch 56: with learning rate 1.52587890625e-06\n",
      "Training loss: 149.7156, Training accuracy: 0.1510\n",
      "Applying pruning: 17.04%\n",
      "Validation loss: 114.9748, Validation accuracy: 0.1927\n",
      "\n",
      "Epoch 57: with learning rate 1.52587890625e-06\n",
      "Training loss: 143.0950, Training accuracy: 0.1410\n",
      "Applying pruning: 17.16%\n",
      "Validation loss: 117.8534, Validation accuracy: 0.1697\n",
      "\n",
      "Epoch 58: with learning rate 1.52587890625e-06\n",
      "Training loss: 144.3068, Training accuracy: 0.1549\n",
      "Applying pruning: 17.29%\n",
      "Validation loss: 129.4278, Validation accuracy: 0.1641\n",
      "\n",
      "Epoch 59: with learning rate 1.52587890625e-06\n",
      "Training loss: 144.0876, Training accuracy: 0.1555\n",
      "Applying pruning: 17.41%\n",
      "Validation loss: 544.7805, Validation accuracy: 0.1000\n",
      "\n",
      "Epoch 60: with learning rate 1.52587890625e-06\n",
      "Training loss: 142.3112, Training accuracy: 0.1382\n",
      "Applying pruning: 17.54%\n",
      "Validation loss: 109.9573, Validation accuracy: 0.1549\n",
      "\n",
      "Epoch 61: with learning rate 1.52587890625e-06\n",
      "Training loss: 141.8879, Training accuracy: 0.1402\n",
      "Applying pruning: 17.66%\n",
      "Validation loss: 110.3255, Validation accuracy: 0.1484\n",
      "Learning rate decayed to 3.814697265625e-07\n",
      "\n",
      "Epoch 62: with learning rate 3.814697265625e-07\n",
      "Training loss: 141.8594, Training accuracy: 0.1401\n",
      "Applying pruning: 17.79%\n",
      "Validation loss: 109.8633, Validation accuracy: 0.1501\n",
      "\n",
      "Epoch 63: with learning rate 3.814697265625e-07\n",
      "Training loss: 140.0847, Training accuracy: 0.1450\n",
      "Applying pruning: 17.91%\n",
      "Validation loss: 109.9035, Validation accuracy: 0.1471\n",
      "\n",
      "Epoch 64: with learning rate 3.814697265625e-07\n",
      "Training loss: 140.0779, Training accuracy: 0.1427\n",
      "Applying pruning: 18.04%\n",
      "Validation loss: 109.9040, Validation accuracy: 0.1492\n",
      "\n",
      "Epoch 65: with learning rate 3.814697265625e-07\n",
      "Training loss: 140.1554, Training accuracy: 0.1432\n",
      "Applying pruning: 18.17%\n",
      "Validation loss: 109.8534, Validation accuracy: 0.1474\n",
      "\n",
      "Epoch 66: with learning rate 3.814697265625e-07\n",
      "Training loss: 140.0806, Training accuracy: 0.1442\n",
      "Applying pruning: 18.29%\n",
      "Validation loss: 109.8064, Validation accuracy: 0.1468\n",
      "\n",
      "Epoch 67: with learning rate 3.814697265625e-07\n",
      "Training loss: 140.0658, Training accuracy: 0.1445\n",
      "Applying pruning: 18.42%\n",
      "Validation loss: 109.7846, Validation accuracy: 0.1469\n",
      "Learning rate decayed to 9.5367431640625e-08\n",
      "\n",
      "Epoch 68: with learning rate 9.5367431640625e-08\n",
      "Training loss: 140.1496, Training accuracy: 0.1442\n",
      "Applying pruning: 18.54%\n",
      "Validation loss: 109.8386, Validation accuracy: 0.1479\n",
      "\n",
      "Epoch 69: with learning rate 9.5367431640625e-08\n",
      "Training loss: 140.1016, Training accuracy: 0.1444\n",
      "Applying pruning: 18.67%\n",
      "Validation loss: 109.8847, Validation accuracy: 0.1493\n",
      "\n",
      "Epoch 70: with learning rate 9.5367431640625e-08\n",
      "Training loss: 140.0827, Training accuracy: 0.1446\n",
      "Applying pruning: 18.79%\n",
      "Validation loss: 109.8752, Validation accuracy: 0.1462\n",
      "\n",
      "Epoch 71: with learning rate 9.5367431640625e-08\n",
      "Training loss: 140.0689, Training accuracy: 0.1460\n",
      "Applying pruning: 18.92%\n",
      "Validation loss: 109.8161, Validation accuracy: 0.1464\n",
      "\n",
      "Epoch 72: with learning rate 9.5367431640625e-08\n",
      "Training loss: 140.0821, Training accuracy: 0.1447\n",
      "Applying pruning: 19.05%\n",
      "Validation loss: 109.8535, Validation accuracy: 0.1508\n",
      "\n",
      "Epoch 73: with learning rate 9.5367431640625e-08\n",
      "Training loss: 140.0738, Training accuracy: 0.1441\n",
      "Applying pruning: 19.17%\n",
      "Validation loss: 109.7947, Validation accuracy: 0.1473\n",
      "Learning rate decayed to 2.384185791015625e-08\n",
      "\n",
      "Epoch 74: with learning rate 2.384185791015625e-08\n",
      "Training loss: 140.1156, Training accuracy: 0.1470\n",
      "Applying pruning: 19.30%\n",
      "Validation loss: 109.8104, Validation accuracy: 0.1471\n",
      "\n",
      "Epoch 75: with learning rate 2.384185791015625e-08\n",
      "Training loss: 140.0531, Training accuracy: 0.1446\n",
      "Applying pruning: 19.42%\n",
      "Validation loss: 109.9370, Validation accuracy: 0.1480\n",
      "\n",
      "Epoch 76: with learning rate 2.384185791015625e-08\n",
      "Training loss: 140.0581, Training accuracy: 0.1438\n",
      "Applying pruning: 19.55%\n",
      "Validation loss: 109.8240, Validation accuracy: 0.1492\n",
      "\n",
      "Epoch 77: with learning rate 2.384185791015625e-08\n",
      "Training loss: 140.0805, Training accuracy: 0.1451\n",
      "Applying pruning: 19.67%\n",
      "Validation loss: 109.9343, Validation accuracy: 0.1481\n",
      "\n",
      "Epoch 78: with learning rate 2.384185791015625e-08\n",
      "Training loss: 140.0803, Training accuracy: 0.1452\n",
      "Applying pruning: 19.80%\n",
      "Validation loss: 109.8559, Validation accuracy: 0.1490\n",
      "\n",
      "Epoch 79: with learning rate 2.384185791015625e-08\n",
      "Training loss: 139.9944, Training accuracy: 0.1448\n",
      "Applying pruning: 19.92%\n",
      "Validation loss: 109.8481, Validation accuracy: 0.1466\n",
      "Learning rate decayed to 5.960464477539063e-09\n",
      "\n",
      "Epoch 80: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0373, Training accuracy: 0.1443\n",
      "Applying pruning: 20.05%\n",
      "Validation loss: 109.7099, Validation accuracy: 0.1469\n",
      "\n",
      "Epoch 81: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0907, Training accuracy: 0.1441\n",
      "Applying pruning: 20.18%\n",
      "Validation loss: 109.8511, Validation accuracy: 0.1465\n",
      "\n",
      "Epoch 82: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.1384, Training accuracy: 0.1448\n",
      "Applying pruning: 20.30%\n",
      "Validation loss: 109.9136, Validation accuracy: 0.1486\n",
      "\n",
      "Epoch 83: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0422, Training accuracy: 0.1448\n",
      "Applying pruning: 20.43%\n",
      "Validation loss: 109.8258, Validation accuracy: 0.1463\n",
      "\n",
      "Epoch 84: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0740, Training accuracy: 0.1451\n",
      "Applying pruning: 20.55%\n",
      "Validation loss: 109.9033, Validation accuracy: 0.1488\n",
      "\n",
      "Epoch 85: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0578, Training accuracy: 0.1455\n",
      "Applying pruning: 20.68%\n",
      "Validation loss: 109.7870, Validation accuracy: 0.1488\n",
      "\n",
      "Epoch 86: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0764, Training accuracy: 0.1449\n",
      "Applying pruning: 20.80%\n",
      "Validation loss: 109.8353, Validation accuracy: 0.1497\n",
      "\n",
      "Epoch 87: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0647, Training accuracy: 0.1453\n",
      "Applying pruning: 20.93%\n",
      "Validation loss: 109.8671, Validation accuracy: 0.1477\n",
      "\n",
      "Epoch 88: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0115, Training accuracy: 0.1459\n",
      "Applying pruning: 21.06%\n",
      "Validation loss: 109.8121, Validation accuracy: 0.1471\n",
      "\n",
      "Epoch 89: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0128, Training accuracy: 0.1452\n",
      "Applying pruning: 21.18%\n",
      "Validation loss: 109.7851, Validation accuracy: 0.1485\n",
      "\n",
      "Epoch 90: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0061, Training accuracy: 0.1447\n",
      "Applying pruning: 21.31%\n",
      "Validation loss: 109.7997, Validation accuracy: 0.1485\n",
      "\n",
      "Epoch 91: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0314, Training accuracy: 0.1445\n",
      "Applying pruning: 21.43%\n",
      "Validation loss: 109.7160, Validation accuracy: 0.1494\n",
      "\n",
      "Epoch 92: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0982, Training accuracy: 0.1448\n",
      "Applying pruning: 21.56%\n",
      "Validation loss: 109.8253, Validation accuracy: 0.1486\n",
      "\n",
      "Epoch 93: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0312, Training accuracy: 0.1458\n",
      "Applying pruning: 21.68%\n",
      "Validation loss: 109.7837, Validation accuracy: 0.1484\n",
      "\n",
      "Epoch 94: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0652, Training accuracy: 0.1446\n",
      "Applying pruning: 21.81%\n",
      "Validation loss: 109.8363, Validation accuracy: 0.1488\n",
      "\n",
      "Epoch 95: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0336, Training accuracy: 0.1444\n",
      "Applying pruning: 21.93%\n",
      "Validation loss: 109.9572, Validation accuracy: 0.1470\n",
      "\n",
      "Epoch 96: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0499, Training accuracy: 0.1435\n",
      "Applying pruning: 22.06%\n",
      "Validation loss: 109.9419, Validation accuracy: 0.1500\n",
      "\n",
      "Epoch 97: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0477, Training accuracy: 0.1453\n",
      "Applying pruning: 22.19%\n",
      "Validation loss: 109.8867, Validation accuracy: 0.1463\n",
      "\n",
      "Epoch 98: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.1025, Training accuracy: 0.1441\n",
      "Applying pruning: 22.31%\n",
      "Validation loss: 109.8734, Validation accuracy: 0.1488\n",
      "\n",
      "Epoch 99: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0495, Training accuracy: 0.1441\n",
      "Applying pruning: 22.44%\n",
      "Validation loss: 109.8250, Validation accuracy: 0.1471\n",
      "\n",
      "Epoch 100: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0659, Training accuracy: 0.1442\n",
      "Applying pruning: 22.56%\n",
      "Validation loss: 109.8779, Validation accuracy: 0.1479\n",
      "\n",
      "Epoch 101: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0288, Training accuracy: 0.1455\n",
      "Applying pruning: 22.69%\n",
      "Validation loss: 109.8280, Validation accuracy: 0.1478\n",
      "\n",
      "Epoch 102: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0668, Training accuracy: 0.1452\n",
      "Applying pruning: 22.81%\n",
      "Validation loss: 109.7857, Validation accuracy: 0.1478\n",
      "\n",
      "Epoch 103: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0825, Training accuracy: 0.1445\n",
      "Applying pruning: 22.94%\n",
      "Validation loss: 109.7944, Validation accuracy: 0.1481\n",
      "\n",
      "Epoch 104: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0578, Training accuracy: 0.1432\n",
      "Applying pruning: 23.07%\n",
      "Validation loss: 109.9144, Validation accuracy: 0.1473\n",
      "\n",
      "Epoch 105: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.1074, Training accuracy: 0.1455\n",
      "Applying pruning: 23.19%\n",
      "Validation loss: 109.7585, Validation accuracy: 0.1485\n",
      "\n",
      "Epoch 106: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.1205, Training accuracy: 0.1444\n",
      "Applying pruning: 23.32%\n",
      "Validation loss: 109.8845, Validation accuracy: 0.1469\n",
      "\n",
      "Epoch 107: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0825, Training accuracy: 0.1456\n",
      "Applying pruning: 23.44%\n",
      "Validation loss: 109.8638, Validation accuracy: 0.1472\n",
      "\n",
      "Epoch 108: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0703, Training accuracy: 0.1442\n",
      "Applying pruning: 23.57%\n",
      "Validation loss: 110.0079, Validation accuracy: 0.1462\n",
      "\n",
      "Epoch 109: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0884, Training accuracy: 0.1441\n",
      "Applying pruning: 23.69%\n",
      "Validation loss: 109.8362, Validation accuracy: 0.1479\n",
      "\n",
      "Epoch 110: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0336, Training accuracy: 0.1454\n",
      "Applying pruning: 23.82%\n",
      "Validation loss: 109.8247, Validation accuracy: 0.1483\n",
      "\n",
      "Epoch 111: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0861, Training accuracy: 0.1445\n",
      "Applying pruning: 23.94%\n",
      "Validation loss: 109.8238, Validation accuracy: 0.1498\n",
      "\n",
      "Epoch 112: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0397, Training accuracy: 0.1454\n",
      "Applying pruning: 24.07%\n",
      "Validation loss: 109.8812, Validation accuracy: 0.1459\n",
      "\n",
      "Epoch 113: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0812, Training accuracy: 0.1449\n",
      "Applying pruning: 24.20%\n",
      "Validation loss: 109.7796, Validation accuracy: 0.1463\n",
      "\n",
      "Epoch 114: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0217, Training accuracy: 0.1443\n",
      "Applying pruning: 24.32%\n",
      "Validation loss: 109.8269, Validation accuracy: 0.1486\n",
      "\n",
      "Epoch 115: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0765, Training accuracy: 0.1454\n",
      "Applying pruning: 24.45%\n",
      "Validation loss: 109.8161, Validation accuracy: 0.1495\n",
      "\n",
      "Epoch 116: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0984, Training accuracy: 0.1437\n",
      "Applying pruning: 24.57%\n",
      "Validation loss: 109.9041, Validation accuracy: 0.1474\n",
      "\n",
      "Epoch 117: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0246, Training accuracy: 0.1449\n",
      "Applying pruning: 24.70%\n",
      "Validation loss: 109.8304, Validation accuracy: 0.1494\n",
      "\n",
      "Epoch 118: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0567, Training accuracy: 0.1452\n",
      "Applying pruning: 24.82%\n",
      "Validation loss: 109.8960, Validation accuracy: 0.1499\n",
      "\n",
      "Epoch 119: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0316, Training accuracy: 0.1449\n",
      "Applying pruning: 24.95%\n",
      "Validation loss: 109.7962, Validation accuracy: 0.1492\n",
      "\n",
      "Epoch 120: with learning rate 5.960464477539063e-09\n",
      "Training loss: 140.0725, Training accuracy: 0.1454\n",
      "Applying pruning: 25.08%\n",
      "Validation loss: 119.0267, Validation accuracy: 0.1440\n",
      "\n",
      "Epoch 121: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1604, Training accuracy: 0.1299\n",
      "Applying pruning: 25.20%\n",
      "Validation loss: 111.0736, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 122: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1752, Training accuracy: 0.1296\n",
      "Applying pruning: 25.33%\n",
      "Validation loss: 111.1022, Validation accuracy: 0.1309\n",
      "\n",
      "Epoch 123: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2144, Training accuracy: 0.1303\n",
      "Applying pruning: 25.45%\n",
      "Validation loss: 111.2134, Validation accuracy: 0.1304\n",
      "\n",
      "Epoch 124: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2028, Training accuracy: 0.1297\n",
      "Applying pruning: 25.58%\n",
      "Validation loss: 111.2246, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 125: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1762, Training accuracy: 0.1297\n",
      "Applying pruning: 25.70%\n",
      "Validation loss: 111.2242, Validation accuracy: 0.1301\n",
      "\n",
      "Epoch 126: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1748, Training accuracy: 0.1288\n",
      "Applying pruning: 25.83%\n",
      "Validation loss: 111.2267, Validation accuracy: 0.1300\n",
      "\n",
      "Epoch 127: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1455, Training accuracy: 0.1298\n",
      "Applying pruning: 25.95%\n",
      "Validation loss: 111.1578, Validation accuracy: 0.1305\n",
      "\n",
      "Epoch 128: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1967, Training accuracy: 0.1301\n",
      "Applying pruning: 26.08%\n",
      "Validation loss: 110.9915, Validation accuracy: 0.1296\n",
      "\n",
      "Epoch 129: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2023, Training accuracy: 0.1300\n",
      "Applying pruning: 26.21%\n",
      "Validation loss: 111.0275, Validation accuracy: 0.1305\n",
      "\n",
      "Epoch 130: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1538, Training accuracy: 0.1279\n",
      "Applying pruning: 26.33%\n",
      "Validation loss: 111.0108, Validation accuracy: 0.1305\n",
      "\n",
      "Epoch 131: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1326, Training accuracy: 0.1294\n",
      "Applying pruning: 26.46%\n",
      "Validation loss: 111.2226, Validation accuracy: 0.1289\n",
      "\n",
      "Epoch 132: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2294, Training accuracy: 0.1285\n",
      "Applying pruning: 26.58%\n",
      "Validation loss: 111.1515, Validation accuracy: 0.1295\n",
      "\n",
      "Epoch 133: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1982, Training accuracy: 0.1299\n",
      "Applying pruning: 26.71%\n",
      "Validation loss: 111.1364, Validation accuracy: 0.1304\n",
      "\n",
      "Epoch 134: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1474, Training accuracy: 0.1294\n",
      "Applying pruning: 26.83%\n",
      "Validation loss: 111.2263, Validation accuracy: 0.1301\n",
      "\n",
      "Epoch 135: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.3192, Training accuracy: 0.1294\n",
      "Applying pruning: 26.96%\n",
      "Validation loss: 111.1037, Validation accuracy: 0.1298\n",
      "\n",
      "Epoch 136: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1736, Training accuracy: 0.1304\n",
      "Applying pruning: 27.09%\n",
      "Validation loss: 111.1853, Validation accuracy: 0.1304\n",
      "\n",
      "Epoch 137: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1075, Training accuracy: 0.1309\n",
      "Applying pruning: 27.21%\n",
      "Validation loss: 111.1256, Validation accuracy: 0.1308\n",
      "\n",
      "Epoch 138: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2073, Training accuracy: 0.1291\n",
      "Applying pruning: 27.34%\n",
      "Validation loss: 110.8859, Validation accuracy: 0.1322\n",
      "\n",
      "Epoch 139: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1454, Training accuracy: 0.1292\n",
      "Applying pruning: 27.46%\n",
      "Validation loss: 111.0292, Validation accuracy: 0.1307\n",
      "\n",
      "Epoch 140: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2276, Training accuracy: 0.1286\n",
      "Applying pruning: 27.59%\n",
      "Validation loss: 111.1301, Validation accuracy: 0.1295\n",
      "\n",
      "Epoch 141: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2376, Training accuracy: 0.1298\n",
      "Applying pruning: 27.71%\n",
      "Validation loss: 110.9767, Validation accuracy: 0.1307\n",
      "\n",
      "Epoch 142: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1518, Training accuracy: 0.1295\n",
      "Applying pruning: 27.84%\n",
      "Validation loss: 111.0619, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 143: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1301, Training accuracy: 0.1306\n",
      "Applying pruning: 27.96%\n",
      "Validation loss: 110.9133, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 144: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1768, Training accuracy: 0.1296\n",
      "Applying pruning: 28.09%\n",
      "Validation loss: 110.9373, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 145: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1690, Training accuracy: 0.1301\n",
      "Applying pruning: 28.22%\n",
      "Validation loss: 111.1830, Validation accuracy: 0.1295\n",
      "\n",
      "Epoch 146: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1676, Training accuracy: 0.1307\n",
      "Applying pruning: 28.34%\n",
      "Validation loss: 110.8351, Validation accuracy: 0.1306\n",
      "\n",
      "Epoch 147: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1453, Training accuracy: 0.1308\n",
      "Applying pruning: 28.47%\n",
      "Validation loss: 111.1129, Validation accuracy: 0.1304\n",
      "\n",
      "Epoch 148: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1497, Training accuracy: 0.1307\n",
      "Applying pruning: 28.59%\n",
      "Validation loss: 111.1388, Validation accuracy: 0.1309\n",
      "\n",
      "Epoch 149: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1592, Training accuracy: 0.1306\n",
      "Applying pruning: 28.72%\n",
      "Validation loss: 110.9259, Validation accuracy: 0.1308\n",
      "\n",
      "Epoch 150: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1712, Training accuracy: 0.1279\n",
      "Applying pruning: 28.84%\n",
      "Validation loss: 110.8570, Validation accuracy: 0.1301\n",
      "\n",
      "Epoch 151: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2530, Training accuracy: 0.1295\n",
      "Applying pruning: 28.97%\n",
      "Validation loss: 111.2418, Validation accuracy: 0.1297\n",
      "\n",
      "Epoch 152: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1192, Training accuracy: 0.1307\n",
      "Applying pruning: 29.10%\n",
      "Validation loss: 111.3805, Validation accuracy: 0.1304\n",
      "\n",
      "Epoch 153: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1992, Training accuracy: 0.1290\n",
      "Applying pruning: 29.22%\n",
      "Validation loss: 111.0059, Validation accuracy: 0.1305\n",
      "\n",
      "Epoch 154: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1609, Training accuracy: 0.1287\n",
      "Applying pruning: 29.35%\n",
      "Validation loss: 111.1135, Validation accuracy: 0.1301\n",
      "\n",
      "Epoch 155: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1241, Training accuracy: 0.1295\n",
      "Applying pruning: 29.47%\n",
      "Validation loss: 111.3103, Validation accuracy: 0.1301\n",
      "\n",
      "Epoch 156: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2029, Training accuracy: 0.1299\n",
      "Applying pruning: 29.60%\n",
      "Validation loss: 111.1887, Validation accuracy: 0.1301\n",
      "\n",
      "Epoch 157: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1698, Training accuracy: 0.1308\n",
      "Applying pruning: 29.72%\n",
      "Validation loss: 111.0774, Validation accuracy: 0.1298\n",
      "\n",
      "Epoch 158: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2339, Training accuracy: 0.1285\n",
      "Applying pruning: 29.85%\n",
      "Validation loss: 111.0984, Validation accuracy: 0.1303\n",
      "\n",
      "Epoch 159: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2371, Training accuracy: 0.1286\n",
      "Applying pruning: 29.97%\n",
      "Validation loss: 110.9501, Validation accuracy: 0.1303\n",
      "\n",
      "Epoch 160: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1093, Training accuracy: 0.1313\n",
      "Applying pruning: 30.10%\n",
      "Validation loss: 111.2492, Validation accuracy: 0.1306\n",
      "\n",
      "Epoch 161: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1978, Training accuracy: 0.1293\n",
      "Applying pruning: 30.23%\n",
      "Validation loss: 111.0834, Validation accuracy: 0.1290\n",
      "\n",
      "Epoch 162: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1178, Training accuracy: 0.1303\n",
      "Applying pruning: 30.35%\n",
      "Validation loss: 111.1599, Validation accuracy: 0.1301\n",
      "\n",
      "Epoch 163: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2018, Training accuracy: 0.1283\n",
      "Applying pruning: 30.48%\n",
      "Validation loss: 111.0841, Validation accuracy: 0.1301\n",
      "\n",
      "Epoch 164: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2410, Training accuracy: 0.1299\n",
      "Applying pruning: 30.60%\n",
      "Validation loss: 111.1022, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 165: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1632, Training accuracy: 0.1291\n",
      "Applying pruning: 30.73%\n",
      "Validation loss: 111.0808, Validation accuracy: 0.1305\n",
      "\n",
      "Epoch 166: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2218, Training accuracy: 0.1296\n",
      "Applying pruning: 30.85%\n",
      "Validation loss: 111.1606, Validation accuracy: 0.1307\n",
      "\n",
      "Epoch 167: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1917, Training accuracy: 0.1301\n",
      "Applying pruning: 30.98%\n",
      "Validation loss: 111.1168, Validation accuracy: 0.1298\n",
      "\n",
      "Epoch 168: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2104, Training accuracy: 0.1288\n",
      "Applying pruning: 31.11%\n",
      "Validation loss: 111.0434, Validation accuracy: 0.1305\n",
      "\n",
      "Epoch 169: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1246, Training accuracy: 0.1316\n",
      "Applying pruning: 31.23%\n",
      "Validation loss: 111.2089, Validation accuracy: 0.1303\n",
      "\n",
      "Epoch 170: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2385, Training accuracy: 0.1297\n",
      "Applying pruning: 31.36%\n",
      "Validation loss: 111.0507, Validation accuracy: 0.1303\n",
      "\n",
      "Epoch 171: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1756, Training accuracy: 0.1299\n",
      "Applying pruning: 31.48%\n",
      "Validation loss: 111.0900, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 172: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1679, Training accuracy: 0.1289\n",
      "Applying pruning: 31.61%\n",
      "Validation loss: 111.2666, Validation accuracy: 0.1297\n",
      "\n",
      "Epoch 173: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1209, Training accuracy: 0.1307\n",
      "Applying pruning: 31.73%\n",
      "Validation loss: 111.2526, Validation accuracy: 0.1295\n",
      "\n",
      "Epoch 174: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2038, Training accuracy: 0.1290\n",
      "Applying pruning: 31.86%\n",
      "Validation loss: 111.0769, Validation accuracy: 0.1303\n",
      "\n",
      "Epoch 175: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2069, Training accuracy: 0.1303\n",
      "Applying pruning: 31.98%\n",
      "Validation loss: 111.2220, Validation accuracy: 0.1300\n",
      "\n",
      "Epoch 176: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1732, Training accuracy: 0.1302\n",
      "Applying pruning: 32.11%\n",
      "Validation loss: 111.1411, Validation accuracy: 0.1309\n",
      "\n",
      "Epoch 177: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1819, Training accuracy: 0.1301\n",
      "Applying pruning: 32.24%\n",
      "Validation loss: 111.1970, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 178: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1682, Training accuracy: 0.1301\n",
      "Applying pruning: 32.36%\n",
      "Validation loss: 111.0346, Validation accuracy: 0.1310\n",
      "\n",
      "Epoch 179: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1822, Training accuracy: 0.1293\n",
      "Applying pruning: 32.49%\n",
      "Validation loss: 111.0017, Validation accuracy: 0.1303\n",
      "\n",
      "Epoch 180: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2296, Training accuracy: 0.1296\n",
      "Applying pruning: 32.61%\n",
      "Validation loss: 111.3020, Validation accuracy: 0.1296\n",
      "\n",
      "Epoch 181: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1380, Training accuracy: 0.1295\n",
      "Applying pruning: 32.74%\n",
      "Validation loss: 111.0715, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 182: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2287, Training accuracy: 0.1299\n",
      "Applying pruning: 32.86%\n",
      "Validation loss: 111.1649, Validation accuracy: 0.1303\n",
      "\n",
      "Epoch 183: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2166, Training accuracy: 0.1292\n",
      "Applying pruning: 32.99%\n",
      "Validation loss: 111.1078, Validation accuracy: 0.1306\n",
      "\n",
      "Epoch 184: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1562, Training accuracy: 0.1306\n",
      "Applying pruning: 33.12%\n",
      "Validation loss: 111.0900, Validation accuracy: 0.1308\n",
      "\n",
      "Epoch 185: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2227, Training accuracy: 0.1305\n",
      "Applying pruning: 33.24%\n",
      "Validation loss: 111.0502, Validation accuracy: 0.1304\n",
      "\n",
      "Epoch 186: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2747, Training accuracy: 0.1301\n",
      "Applying pruning: 33.37%\n",
      "Validation loss: 111.2137, Validation accuracy: 0.1300\n",
      "\n",
      "Epoch 187: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2231, Training accuracy: 0.1292\n",
      "Applying pruning: 33.49%\n",
      "Validation loss: 111.0923, Validation accuracy: 0.1296\n",
      "\n",
      "Epoch 188: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2276, Training accuracy: 0.1289\n",
      "Applying pruning: 33.62%\n",
      "Validation loss: 111.2760, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 189: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2244, Training accuracy: 0.1296\n",
      "Applying pruning: 33.74%\n",
      "Validation loss: 111.5027, Validation accuracy: 0.1304\n",
      "\n",
      "Epoch 190: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1504, Training accuracy: 0.1302\n",
      "Applying pruning: 33.87%\n",
      "Validation loss: 111.1430, Validation accuracy: 0.1304\n",
      "\n",
      "Epoch 191: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1382, Training accuracy: 0.1301\n",
      "Applying pruning: 33.99%\n",
      "Validation loss: 111.3348, Validation accuracy: 0.1300\n",
      "\n",
      "Epoch 192: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1872, Training accuracy: 0.1295\n",
      "Applying pruning: 34.12%\n",
      "Validation loss: 111.3396, Validation accuracy: 0.1311\n",
      "\n",
      "Epoch 193: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1086, Training accuracy: 0.1302\n",
      "Applying pruning: 34.25%\n",
      "Validation loss: 111.1355, Validation accuracy: 0.1299\n",
      "\n",
      "Epoch 194: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1853, Training accuracy: 0.1302\n",
      "Applying pruning: 34.37%\n",
      "Validation loss: 111.1279, Validation accuracy: 0.1307\n",
      "\n",
      "Epoch 195: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1279, Training accuracy: 0.1295\n",
      "Applying pruning: 34.50%\n",
      "Validation loss: 111.0564, Validation accuracy: 0.1302\n",
      "\n",
      "Epoch 196: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1354, Training accuracy: 0.1304\n",
      "Applying pruning: 34.62%\n",
      "Validation loss: 111.1010, Validation accuracy: 0.1307\n",
      "\n",
      "Epoch 197: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2217, Training accuracy: 0.1296\n",
      "Applying pruning: 34.75%\n",
      "Validation loss: 111.1637, Validation accuracy: 0.1300\n",
      "\n",
      "Epoch 198: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.1627, Training accuracy: 0.1299\n",
      "Applying pruning: 34.87%\n",
      "Validation loss: 110.8843, Validation accuracy: 0.1309\n",
      "\n",
      "Epoch 199: with learning rate 5.960464477539063e-09\n",
      "Training loss: 141.2099, Training accuracy: 0.1304\n",
      "Applying pruning: 35.00%\n",
      "Validation loss: 110.8939, Validation accuracy: 0.1293\n",
      "\n",
      "==================================================\n",
      "==> Optimization finished! Best validation accuracy: 0.7654\n"
     ]
    }
   ],
   "source": [
    "# ===== Set up the loss function and optimizer =====\n",
    "\n",
    "\n",
    "## loss function\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "## Add optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=LR_PATIENCE, factor=LR_FACTOR)\n",
    "\n",
    "## Pruning Array\n",
    "PruningArray = np.linspace(0.10, 0.35, EPOCHS)\n",
    "\n",
    "\n",
    "# ===== Start the training process =====\n",
    "\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"==> Training starts!\")\n",
    "print(\"=\"*50)\n",
    "for i in range(0, EPOCHS):    \n",
    "    ## switch to train mode\n",
    "    net.train()\n",
    "\n",
    "    ## print the Epoch and learning rate\n",
    "    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {i}: with learning rate {current_learning_rate}\")\n",
    "    \n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    ## Train the model\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        ### copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        ### compute the output and loss\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets.long())\n",
    "        \n",
    "        ### zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ### apply gradient and update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### count the number of correctly predicted samples in the current batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_examples += targets.size(0)\n",
    "        correct_examples += (predicted == targets).sum().item()\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "                \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "\n",
    "    if ENABLE_PRUNING:\n",
    "        net.prune_weights(PruningArray[i])\n",
    "        print(\"Applying pruning: \" + f\"{PruningArray[i]:.2%}\" )\n",
    "\n",
    "    ## Validate on the validation dataset\n",
    "    ## switch to eval mode\n",
    "    net.eval()\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    ## disable gradient during validation, which can save GPU memory\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            ### copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            ### compute the output and loss\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets.long())\n",
    "            \n",
    "            ### count the number of correctly predicted samples in the current batch\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_examples += targets.size(0)\n",
    "            correct_examples += (predicted == targets).sum().item()\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "\n",
    "    ## decay learning rate\n",
    "    previous_learning_rate = current_learning_rate\n",
    "    scheduler.step(val_loss)\n",
    "    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "    if previous_learning_rate != current_learning_rate:\n",
    "        print(f\"Learning rate decayed to {current_learning_rate}\")\n",
    "    \n",
    "    ## save the model checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "           os.makedirs(CHECKPOINT_FOLDER)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'state_dict': net.state_dict(),\n",
    "                'epoch': i,\n",
    "                'lr': current_learning_rate}\n",
    "        saveName = 'CNN_quantizeTrained' if ENABLE_QUANTIZATION else 'CNN'\n",
    "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, saveName + '.pth'))\n",
    "    print('')\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in init_conv: 0.00%\n",
      "Sparsity in features.0.0.conv1: 99.77%\n",
      "Sparsity in features.0.0.conv2: 99.96%\n",
      "Sparsity in features.0.0.shortcut.0: 0.00%\n",
      "Sparsity in features.0.1.conv1: 99.96%\n",
      "Sparsity in features.0.1.conv2: 99.96%\n",
      "Sparsity in features.0.2.conv1: 99.96%\n",
      "Sparsity in features.0.2.conv2: 99.96%\n",
      "Sparsity in features.1.0.conv1: 99.98%\n",
      "Sparsity in features.1.0.conv2: 99.99%\n",
      "Sparsity in features.1.0.shortcut.0: 0.00%\n",
      "Sparsity in features.1.1.conv1: 99.99%\n",
      "Sparsity in features.1.1.conv2: 99.99%\n",
      "Sparsity in features.1.2.conv1: 99.99%\n",
      "Sparsity in features.1.2.conv2: 99.99%\n",
      "Sparsity in features.2.0.conv1: 99.99%\n",
      "Sparsity in features.2.0.conv2: 100.00%\n",
      "Sparsity in features.2.0.shortcut.0: 0.00%\n",
      "Sparsity in features.2.1.conv1: 100.00%\n",
      "Sparsity in features.2.1.conv2: 100.00%\n",
      "Sparsity in features.2.2.conv1: 100.00%\n",
      "Sparsity in features.2.2.conv2: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def check_sparsity(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # Ensure the weight is on CPU for sparsity check\n",
    "            weight = module.weight.data\n",
    "            if weight.is_sparse:\n",
    "                weight = weight.to_dense()  # Convert to dense if sparse for computation\n",
    "            weight = weight.cpu()  # Move to CPU\n",
    "            sparsity = float(torch.sum(weight == 0)) / float(weight.nelement())\n",
    "            print(f\"Sparsity in {name}: {sparsity * 100:.2f}%\")\n",
    "\n",
    "check_sparsity(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Size: 0.34 MB\n"
     ]
    }
   ],
   "source": [
    "quantizedModel = True\n",
    "net.to('cpu')\n",
    "device = 'cpu'\n",
    "torch.quantization.convert(net.eval(), inplace=True)\n",
    "\n",
    "saveName = 'CNN' + '_quantized' if quantizedModel else 'CNN'\n",
    "torch.save(net.state_dict(), os.path.join(CHECKPOINT_FOLDER, saveName + '.pth'))\n",
    "\n",
    "# Check the size of the quantized model\n",
    "model_size_bytes = os.path.getsize(CHECKPOINT_FOLDER + '/' + saveName + '.pth')\n",
    "model_size_mb = model_size_bytes / (1024 * 1024)  # Convert bytes to megabytes\n",
    "print(f\"Quantized Model Size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute 'weight_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m---> 21\u001b[0m evaluate_model(net, val_loader)\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, data_loader)\u001b[0m\n\u001b[0;32m      8\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m      9\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     11\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 179\u001b[0m, in \u001b[0;36mQuantizablePrunableCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    178\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)\n\u001b[1;32m--> 179\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m    180\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(x)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[1;32mIn[6], line 160\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 160\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m    161\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madaptive_pool(x)\n\u001b[0;32m    162\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     66\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 67\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m     68\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     69\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1550\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1545\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1546\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward pre-hook must return None or a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1547\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_kwargs_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1548\u001b[0m             )\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     args_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args)\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1552\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\utils\\prune.py:30\u001b[0m, in \u001b[0;36mBasePruningMethod.__call__\u001b[1;34m(self, module, inputs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, module, inputs):\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Multiply the mask into original tensor and store the result.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    Multiplies the mask (stored in ``module[name + '_mask']``)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m        inputs: not used.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_mask(module))\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\utils\\prune.py:68\u001b[0m, in \u001b[0;36mBasePruningMethod.apply_mask\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# to carry out the multiplication, the mask needs to have been computed,\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# so the pruning method must know what tensor it's operating on\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has to be pruned\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# this gets set in apply()\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_orig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m pruned_tensor \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39morig\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m*\u001b[39m orig\n",
      "File \u001b[1;32mc:\\Users\\barto\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Conv2d' object has no attribute 'weight_mask'"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss += criterion(outputs, labels).item() * images.size(0)\n",
    "    \n",
    "    avg_loss = loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "evaluate_model(net, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
